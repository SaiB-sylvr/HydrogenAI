# üèóÔ∏è Services Directory - Microservices Architecture

## üìã **Overview**
This directory contains all microservices that form the HydrogenAI platform. Each service is independently deployable, containerized, and communicates via well-defined APIs and event systems.

## üèõÔ∏è **Architecture Pattern**
- **Microservices Architecture**: Each service has distinct responsibilities
- **Domain-Driven Design**: Services organized around business capabilities
- **Event-Driven Communication**: NATS/Redis event bus for loose coupling
- **API Gateway Pattern**: Kong gateway for unified API access
- **Shared Libraries**: Common functionality in `shared/` directory

## üì¶ **Service Structure**

### **üéØ Orchestrator Service (`orchestrator/`)**
- **Purpose**: Main AI orchestration and query processing engine
- **Responsibilities**:
  - AI-powered query classification and routing
  - Multi-provider AI management (Groq, OpenAI, Anthropic)
  - Workflow execution and agent coordination
  - State management and caching
  - WebSocket connections for real-time communication
- **Key Components**:
  - `main.py` - Core FastAPI application (3,626 lines)
  - `agents/` - AI agent system with LangChain integration
  - `core/` - Infrastructure components (state, events, circuits)
- **Interactions**:
  - **‚Üí MCP Server**: Tool execution via HTTP
  - **‚Üí Redis**: Caching and state management
  - **‚Üí NATS**: Event publishing/subscribing
  - **‚Üí Kong**: Receives API requests
  - **‚Üí Shared Services**: AI providers and caching

### **üõ†Ô∏è MCP Server (`mcp-server/`)**
- **Purpose**: MongoDB operations and RAG (Retrieval-Augmented Generation) functionality
- **Responsibilities**:
  - MongoDB CRUD operations and aggregations
  - Document embedding and vector storage
  - RAG pipeline implementation
  - Plugin system for extensible tools
  - Database optimization and statistics
- **Key Components**:
  - `main.py` - FastAPI server with tool execution
  - `plugin_manager.py` - Dynamic plugin loading
  - `tool_registry.py` - Centralized tool management
- **Interactions**:
  - **‚Üê Orchestrator**: Receives tool execution requests
  - **‚Üí MongoDB**: Database operations
  - **‚Üí Qdrant**: Vector storage for RAG
  - **‚Üí Plugin System**: Extensible tool ecosystem

### **üìö Shared Services (`shared/`)**
- **Purpose**: Common functionality shared across all services
- **Responsibilities**:
  - AI provider management with fallback logic
  - Redis-based intelligent caching system
  - Data models and validation
  - Configuration validation
  - Common utilities
- **Key Components**:
  - `ai_provider_manager.py` - Multi-provider AI orchestration
  - `ai_cache.py` - Semantic caching with conversation awareness
  - `models.py` - Pydantic data models
  - `config_validator.py` - Configuration validation
- **Interactions**:
  - **‚Üê All Services**: Import shared functionality
  - **‚Üí External APIs**: Groq, OpenAI, Anthropic
  - **‚Üí Redis**: Caching operations

### **üê≥ Base Services (`base/`)**
- **Purpose**: Base Docker image and common dependencies
- **Responsibilities**:
  - Common Python dependencies
  - Base container configuration
  - Shared system packages
- **Usage**: Parent image for all service containers

### **üåê Gateway Services (`gateway/`)**
- **Purpose**: Kong API gateway extensions
- **Responsibilities**:
  - Circuit breaker implementation in Lua
  - Custom Kong plugins
  - Advanced routing logic
- **Interactions**:
  - **‚Üê Kong**: Lua script execution
  - **‚Üí All Services**: Request routing and protection

## üîÑ **Service Communication Patterns**

### **Synchronous Communication**
```
Client ‚Üí Kong Gateway ‚Üí Orchestrator ‚Üí MCP Server
                     ‚Üì
               Shared Services
```

### **Asynchronous Communication**
```
Orchestrator ‚Üí NATS Event Bus ‚Üí MCP Server
           ‚Üì
    Redis Cache ‚Üê Shared Services
```

### **Data Flow**
```
1. API Request arrives at Kong Gateway
2. Kong routes to Orchestrator service
3. Orchestrator classifies query using AI
4. Orchestrator calls MCP Server for tools
5. MCP Server executes MongoDB/RAG operations
6. Results flow back through the chain
7. Shared Services handle caching and AI providers
```

## üß† **AI Intelligence Integration**

### **Multi-Provider AI System**
- **Primary**: Groq (fast, cost-effective)
- **Fallback**: OpenAI, Anthropic
- **Local**: Pattern-based classification
- **Features**: Rate limiting, automatic failover, cost optimization

### **Caching Strategy**
- **Semantic Caching**: AI-powered cache key generation
- **Conversation Cache**: Context-aware caching
- **TTL Management**: Dynamic expiration policies
- **Performance**: 3-5x query performance improvement

## üîß **Development Guidelines**

### **Adding New Services**
1. Create service directory following naming convention
2. Include `Dockerfile`, `requirements.txt`, `__init__.py`
3. Implement health check endpoints
4. Add service to `docker-compose.yml`
5. Configure Kong routing if needed
6. Update shared models if required

### **Service Dependencies**
- All services depend on `shared/` libraries
- Import shared components: `from services.shared import *`
- Use dependency injection for external services
- Implement graceful degradation for failures

### **Error Handling**
- Circuit breaker pattern for external calls
- Retry logic with exponential backoff
- Comprehensive logging with correlation IDs
- Health check endpoints for monitoring

## üìä **Monitoring and Observability**

### **Health Checks**
- Each service exposes `/health` endpoint
- Dependency health verification
- Resource usage monitoring
- Service discovery support

### **Logging**
- Structured JSON logging
- Correlation ID tracking
- Error aggregation
- Performance metrics

### **Metrics**
- Prometheus metrics collection
- Service-specific KPIs
- Resource utilization tracking
- Business metrics

## üöÄ **Deployment Considerations**

### **Container Strategy**
- Multi-stage Docker builds
- Optimized layer caching
- Security-hardened base images
- Resource limits and requests

### **Scaling**
- Horizontal pod autoscaling
- Load balancer configuration
- Database connection pooling
- Cache distribution

### **Configuration**
- Environment-based configuration
- Secret management
- Feature flags
- Configuration validation

## üîê **Security**

### **Service-to-Service Communication**
- mTLS for production environments
- API key validation
- Request/response validation
- Rate limiting per service

### **Data Protection**
- Input sanitization
- SQL/NoSQL injection prevention
- Credential management
- Audit logging

## üìà **Performance Optimization**

### **Caching Strategy**
- Multi-level caching (Redis, in-memory)
- Cache warming strategies
- Intelligent cache invalidation
- Cache hit ratio monitoring

### **Database Optimization**
- Connection pooling
- Query optimization
- Index management
- Read/write separation

### **Resource Management**
- Memory leak prevention
- Connection cleanup
- Graceful shutdown
- Resource pooling

---

## üéØ **Quick Start for Developers**

1. **Understand the Architecture**: Review this README and service-specific documentation
2. **Set Up Environment**: Copy `.env.example` to `.env` and configure
3. **Start Services**: `docker-compose up -d`
4. **Verify Health**: Check all `/health` endpoints
5. **Run Tests**: Execute test suite to verify functionality
6. **Read Service Docs**: Review each service's README for specific details

## ü§ù **Contributing**

- Follow service-specific contribution guidelines
- Ensure all tests pass before submitting
- Update documentation for any architectural changes
- Maintain backward compatibility in shared services
- Use semantic versioning for service updates

This services architecture provides a robust, scalable, and maintainable foundation for the HydrogenAI platform's AI-powered data orchestration capabilities.
