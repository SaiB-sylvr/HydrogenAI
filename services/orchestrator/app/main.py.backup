"""
Intelligent Agent-Based Orchestrator with Enhanced Resource Management
Uses AI agents for all decision making with comprehensive resource cleanup
"""
from fastapi import FastAPI, HTTPException, BackgroundTasks, WebSocket, WebSocketDisconnect
from fastapi.responses import JSONResponse, StreamingResponse
from contextlib import asynccontextmanager
import asyncio
from typing import Dict, Any, List, Optional, AsyncGenerator
import os
import logging
import json
import time
import hashlib
from datetime import datetime, timedelta
import uuid
import httpx

# Load environment variables early
def load_env_file(env_path: str = ".env"):
    """Load environment variables from .env file"""
    if os.path.exists(env_path):
        with open(env_path, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    # Only set if not already in environment
                    if key not in os.environ:
                        os.environ[key] = value
        print(f"âœ… Loaded environment variables from {env_path}")

# Load .env file first
load_env_file()

# Import enhanced components
from app.core.dependency_manager import dependency_manager
from app.core.enhanced_config import get_config_manager, get_timeout, get_cache_ttl
from app.core.resource_manager import get_resource_manager, ResourceType
from app.agents.agent_system import AgentRuntime
from app.core.state_manager import StateManager
from app.core.circuit_breaker import CircuitBreaker
from app.workflow_engine import WorkflowEngine

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Import shared services after logger is configured
import sys
sys.path.append('/app')
sys.path.append('/app/services')
sys.path.append('/app/services/shared')

try:
    from services.shared.config_validator import ConfigValidator
    from services.shared.ai_provider_manager import AIProviderManager
    from services.shared.ai_cache import AIResponseCache
    from services.shared.models import QueryRequest, QueryResponse
    logger.info("âœ… Successfully imported shared services")
except ImportError as e:
    logger.warning(f"Failed to import shared services: {e}")
    # Fallback imports for development
    try:
        from shared.config_validator import ConfigValidator
        from shared.ai_provider_manager import AIProviderManager
        from shared.ai_cache import AIResponseCache
        from shared.models import QueryRequest, QueryResponse
        logger.info("âœ… Successfully imported shared services (fallback path)")
    except ImportError:
        logger.error("Could not import shared services. Please ensure services/shared is in Python path.")
        ConfigValidator = None
        AIProviderManager = None
        AIResponseCache = None

# Enhanced Configuration
config_manager = get_config_manager()
resource_manager = get_resource_manager()

class Config:
    LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")
    MCP_SERVER_URL = os.getenv("MCP_SERVER_URL", "http://mcp-server:8000")
    MONGO_URI = os.getenv("MONGO_URI")
    MONGO_DB_NAME = os.getenv("MONGO_DB_NAME")
    REDIS_URL = os.getenv("REDIS_URL", "redis://redis:6379")
    
    # Enhanced performance settings with dynamic configuration
    @property
    def QUERY_TIMEOUT(self):
        return get_timeout("query")
    
    @property
    def RESULT_CACHE_TTL(self):
        return get_cache_ttl("result")
    
    @property
    def STATE_TTL(self):
        return get_cache_ttl("state")

config = Config()

# Global components - Enhanced with AI provider management
app_state = {
    "ready": False,
    "agent_runtime": None,
    "state_manager": None,
    "circuit_breaker": None,
    "workflow_engine": None,
    "active_queries": {},  # Track active queries
    "websocket_connections": [],
    "ai_provider_manager": None,
    "ai_cache": None,
    "config_validator": None
}

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Enhanced application lifespan with intelligent resource management"""
    try:
        logger.info("ðŸš€ Starting Enhanced Hydrogen Orchestrator...")
        
        # Validate configuration first
        if ConfigValidator:
            app_state["config_validator"] = ConfigValidator()
            is_valid = app_state["config_validator"].validate_all()
            if not is_valid:
                logger.error("âŒ Configuration validation failed")
                raise RuntimeError("Invalid configuration")
            logger.info("âœ… Configuration validation passed")
        
        # Initialize AI provider manager
        if AIProviderManager:
            app_state["ai_provider_manager"] = AIProviderManager()
            await app_state["ai_provider_manager"].initialize()
            logger.info("âœ… AI provider manager initialized with multi-provider support")
        
        # Initialize AI response cache
        if AIResponseCache:
            app_state["ai_cache"] = AIResponseCache(config.REDIS_URL)
            await app_state["ai_cache"].initialize()
            logger.info("âœ… AI response cache initialized")
        
        # Initialize resource management
        await resource_manager.start()
        logger.info("âœ… Resource manager initialized")
        
        # Configure and initialize dependencies with AI provider manager
        if app_state.get("ai_provider_manager"):
            # Use AI provider manager instead of hardcoded Groq
            dependency_manager.register_service("llm", app_state["ai_provider_manager"], {
                "fallback_enabled": True,
                "cache_enabled": True
            })
        else:
            # Fallback to old configuration if AI provider manager not available
            dependency_manager.register_service("llm", None, {
                "api_key": os.getenv("GROQ_API_KEY"),
                "model": os.getenv("GROQ_MODEL_NAME"),
                "base_url": os.getenv("GROQ_API_BASE")
            })
        
        dependency_manager.register_service("event_bus", None, {
            "nats_url": os.getenv("EVENT_BUS_URL", "nats://nats:4222"),
            "redis_url": config.REDIS_URL
        })
        
        # Initialize all dependencies with intelligent fallbacks
        init_results = await dependency_manager.initialize_all()
        logger.info(f"Dependency initialization: {init_results}")
        
        # Initialize state manager with resource tracking
        app_state["state_manager"] = StateManager()
        async with resource_manager.managed_resource(
            app_state["state_manager"], ResourceType.CONNECTION
        ) as state_manager:
            await state_manager.initialize()
            app_state["state_manager"] = state_manager
            # Also set redis client in app_state for consistent access
            app_state["redis"] = state_manager.redis_client
        logger.info("âœ… State manager initialized with resource tracking")
        
        # Initialize enhanced circuit breaker
        app_state["circuit_breaker"] = CircuitBreaker(
            failure_threshold=config_manager.monitoring_config.error_rate_threshold * 100,
            recovery_timeout=30
        )
        
        # Initialize agent runtime with AI provider manager
        app_state["agent_runtime"] = AgentRuntime()
        
        # Set AI provider manager if available
        if app_state.get("ai_provider_manager"):
            app_state["agent_runtime"].set_ai_provider_manager(app_state["ai_provider_manager"])
            logger.info("âœ… Agent runtime configured with AI provider manager")
        
        # Set LLM service as fallback
        llm_service = dependency_manager.get_service("llm")
        if llm_service:
            app_state["agent_runtime"].set_llm_service(llm_service)
        
        await app_state["agent_runtime"].initialize()
        logger.info("âœ… AI Agent runtime initialized with enhanced AI services")
        
        # Initialize event bus with resource management
        event_bus_service = dependency_manager.get_service("event_bus")
        if event_bus_service:
            app_state["event_bus"] = event_bus_service
            logger.info("âœ… Event bus initialized with intelligent fallbacks")
        else:
            # Fallback to basic event bus
            from app.core.event_bus import EventBus
            app_state["event_bus"] = EventBus()
            try:
                await app_state["event_bus"].connect()
                logger.info("âœ… Event bus initialized with basic implementation")
            except Exception as e:
                logger.warning(f"Event bus initialization failed: {e}")
        
        # Initialize workflow engine with enhanced configuration
        app_state["workflow_engine"] = WorkflowEngine(
            event_bus=app_state["event_bus"],
            state_manager=app_state["state_manager"],
            circuit_breaker=app_state["circuit_breaker"]
        )
        app_state["workflow_engine"].set_agent_runtime(app_state["agent_runtime"])
        app_state["workflow_engine"].load_workflows("/app/config/workflows")
        
        # Create connection pools for external services
        if config.MONGO_URI:
            mongo_pool = resource_manager.create_connection_pool(
                "mongodb",
                lambda: create_mongo_connection(),
                min_size=5,
                max_size=config_manager.resource_config.mongo_pool_size
            )
            app_state["mongo_pool"] = mongo_pool
        
        # Setup periodic health checks
        asyncio.create_task(periodic_health_check())
        
        app_state["ready"] = True
        
        # Log system status
        health_status = resource_manager.get_health_status()
        logger.info("ðŸŽ¯ Enhanced Orchestrator ready!")
        logger.info(f"ðŸ“Š System Health: {health_status['healthy']}")
        logger.info(f"ðŸ’¾ Memory Usage: {health_status['metrics']['memory_usage_mb']:.1f}MB")
        logger.info(f"ðŸ”— Active Connections: {health_status['metrics']['active_connections']}")
        
        yield
        
    finally:
        # Enhanced cleanup with resource management
        logger.info("ðŸ›‘ Shutting down Enhanced Orchestrator...")
        app_state["ready"] = False
        
        # Cleanup in reverse order of initialization
        if app_state.get("workflow_engine"):
            try:
                await app_state["workflow_engine"].cleanup()
            except Exception as e:
                logger.error(f"Workflow engine cleanup failed: {e}")
        
        if app_state.get("event_bus"):
            try:
                await app_state["event_bus"].cleanup()
            except Exception as e:
                logger.error(f"Event bus cleanup failed: {e}")
        
        if app_state.get("agent_runtime"):
            try:
                await app_state["agent_runtime"].cleanup()
            except Exception as e:
                logger.error(f"Agent runtime cleanup failed: {e}")
        
        if app_state.get("state_manager"):
            try:
                await app_state["state_manager"].cleanup()
            except Exception as e:
                logger.error(f"State manager cleanup failed: {e}")
        
        # Cleanup AI services
        if app_state.get("ai_cache"):
            try:
                await app_state["ai_cache"].cleanup()
                logger.info("âœ… AI cache cleanup completed")
            except Exception as e:
                logger.error(f"AI cache cleanup failed: {e}")
        
        if app_state.get("ai_provider_manager"):
            try:
                await app_state["ai_provider_manager"].cleanup()
                logger.info("âœ… AI provider manager cleanup completed")
            except Exception as e:
                logger.error(f"AI provider manager cleanup failed: {e}")
        
        # Cleanup dependency manager
        await dependency_manager.cleanup_all()
        
        # Stop resource management last
        await resource_manager.stop()
        
        logger.info("âœ… Enhanced cleanup completed")

async def create_mongo_connection():
    """Create MongoDB connection with proper configuration"""
    from motor.motor_asyncio import AsyncIOMotorClient
    return AsyncIOMotorClient(config.MONGO_URI)

async def periodic_health_check():
    """Periodic health check for all systems"""
    while app_state.get("ready", False):
        try:
            # Check dependencies
            health_results = await dependency_manager.health_check_all()
            
            # Check resource usage
            resource_health = resource_manager.get_health_status()
            
            # Log if any issues
            if not resource_health["healthy"]:
                logger.warning(f"Resource health issues: {resource_health}")
            
            unhealthy_deps = [name for name, healthy in health_results.items() if not healthy]
            if unhealthy_deps:
                logger.warning(f"Unhealthy dependencies: {unhealthy_deps}")
            
            await asyncio.sleep(60)  # Check every minute
            
        except Exception as e:
            logger.error(f"Health check failed: {e}")
            await asyncio.sleep(60)

app = FastAPI(
    title="Hydrogen AI Orchestrator",
    version="3.0.0",
    description="Intelligent Query Processing with AI Agents",
    lifespan=lifespan
)

class IntelligentQueryProcessor:
    """AI-powered query processor - no hard-coded rules!"""
    
    def __init__(self, agent_runtime: AgentRuntime, state_manager: StateManager):
        self.agent_runtime = agent_runtime
        self.state_manager = state_manager
        self.understanding_agent = None
        # Initialize Redis client from app state
        self.redis = app_state.get("redis") or state_manager.redis_client
    
    async def process_query(self, query: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """COMPLETELY DYNAMIC AI processing - NO CACHING, FRESH ANALYSIS EVERY TIME"""
        request_id = str(uuid.uuid4())
        
        try:
            logger.info(f"ðŸš€ Processing FRESH query (no cache): '{query[:50]}...'")
            
            # Skip ALL caching - always do fresh analysis
            logger.info(f"ðŸ¤– Starting fresh AI-powered dynamic analysis...")
            
            # Step 1: Fresh AI-powered data analysis (no caching)
            ai_analysis = await self._ai_powered_data_analysis(query)
            
            # Step 2: Generate human-readable response from live data
            if ai_analysis.get("success"):
                human_response = ai_analysis["ai_response"]
                
                return {
                    "request_id": request_id,
                    "status": "completed", 
                    "understanding": {
                        "steps": [f"Analyzed user intent: {ai_analysis['user_intent']['primary_intent']}"],
                        "estimated_time": 0,
                        "resources_needed": ai_analysis['live_data_summary']['collections_analyzed'],
                        "optimization_hints": [f"Used {ai_analysis['strategy']['analysis_type']} strategy"]
                    },
                    "approach": {
                        "steps": [f"Connected to live database", f"Fetched data from {len(ai_analysis['live_data_summary']['collections_analyzed'])} collections"],
                        "estimated_time": 0,
                        "resources_needed": ai_analysis['live_data_summary']['collections_analyzed'],
                        "optimization_hints": ["Real-time database analysis", "No caching used"]
                    },
                    "human_response": human_response,
                    "result": {
                        "success": True,
                        "results": ai_analysis['live_data_summary'],
                        "messages": [f"Analyzed {ai_analysis['live_data_summary']['total_records']} records"],
                        "metrics": ai_analysis['live_data_summary']
                    },
                    "from_cache": False,  # Always false - no caching
                    "is_fresh_analysis": True,
                    "timestamp": datetime.utcnow().isoformat()
                }
            else:
                # Error case
                error_msg = ai_analysis.get("error", "Unknown error")
                return {
                    "request_id": request_id,
                    "status": "error",
                    "error": error_msg,
                    "human_response": f"I encountered an error while analyzing your database: {error_msg}",
                    "from_cache": False,
                    "timestamp": datetime.utcnow().isoformat()
                }
                
        except Exception as e:
            logger.error(f"Error in dynamic query processing: {e}")
            return {
                "request_id": request_id,
                "status": "error",
                "error": str(e),
                "human_response": f"I encountered an error while processing your query: {str(e)}",
                "from_cache": False,
                "timestamp": datetime.utcnow().isoformat()
            }
            
            if is_streaming:
                # For streaming, save state and return immediately
                await self.state_manager.update_state(request_id, {
                    "status": "streaming",
                    "result": result,
                    "approach": approach,
                    "stream_url": f"/api/stream/{request_id}",
                    "completed_at": datetime.utcnow().isoformat()
                })
                
                return {
                    "request_id": request_id,
                    "status": "streaming",
                    "understanding": understanding,
                    "approach": approach,
                    "stream_url": f"/api/stream/{request_id}",
                    "message": "Large dataset detected. Use the streaming endpoint to retrieve results efficiently.",
                    "result": result
                }
            
            # Step 5: Format response in human language (non-streaming only)
            # Production-ready human response generation
            logger.info(f"ðŸ”§ About to generate production response for query: '{query}'")
            logger.info(f"ðŸ”§ Result structure keys: {list(result.keys()) if isinstance(result, dict) else 'Not a dict'}")
            
            # Extract data context for caching
            actual_data = await self._fetch_real_data_for_query(query)
            
            human_response = await self._generate_production_response(query, result)
            logger.info(f"âœ… Generated production human response: {human_response}")
            logger.info(f"ðŸ”§ Type of human_response: {type(human_response)}")
            
            # Save final result with extended TTL
            await self.state_manager.update_state(request_id, {
                "status": "completed",
                "result": result,
                "human_response": human_response,
                "approach": approach,
                "completed_at": datetime.utcnow().isoformat()
            })
            
            # Also save in Redis with proper TTL
            await self.state_manager.redis_client.setex(
                f"result:{request_id}",
                config.RESULT_CACHE_TTL,
                json.dumps({
                    "query": query,
                    "result": result,
                    "human_response": human_response,
                    "timestamp": datetime.utcnow().isoformat()
                })
            )
            
            # Save to conversational cache with analysis context
            await self._save_conversational_cache(query, result, human_response, actual_data)
            
            # Save to query cache for future identical queries
            import hashlib
            query_hash = hashlib.md5(query.encode()).hexdigest()
            await self.state_manager.redis_client.setex(
                f"query_cache:{query_hash}",
                config.RESULT_CACHE_TTL,
                json.dumps({
                    "request_id": request_id,
                    "status": "completed",
                    "understanding": understanding,
                    "approach": approach,
                    "human_response": human_response,
                    "result": result,
                    "cached": False,
                    "timestamp": datetime.utcnow().isoformat()
                })
            )
            logger.info(f"ðŸ’¾ Cached query result: {query[:50]}...")
            
            return {
                "request_id": request_id,
                "status": "completed",
                "understanding": understanding,
                "approach": approach,
                "human_response": human_response,
                "result": result,
                "from_cache": False
            }
            
        except Exception as e:
            logger.error(f"Query processing failed: {e}")
            await self.state_manager.update_state(request_id, {
                "status": "failed",
                "error": str(e)
            })
            raise
    
    async def _understand_query(self, query: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """Use AI to understand query intent with enhanced provider management and caching"""
        
        # Check cache first if available
        cache_key = f"query_understanding:{hashlib.md5(query.encode()).hexdigest()}"
        if app_state.get("ai_cache"):
            cached_result = await app_state["ai_cache"].get(cache_key)
            if cached_result:
                logger.info("âœ… Using cached query understanding")
                return cached_result
        
        # Use the query planning agent to understand intent
        planner = self.agent_runtime.get_agent("query_planner")
        if not planner:
            # Fallback to basic understanding
            fallback_result = {
                "intent": "unknown",
                "confidence": 0.5,
                "entities": [],
                "complexity": "simple"
            }
            
            # Cache fallback result
            if app_state.get("ai_cache"):
                await app_state["ai_cache"].set(cache_key, fallback_result, ttl=300)  # 5 minutes
            
            return fallback_result
        
        understanding_task = f"""
        Analyze this query and provide understanding:
        Query: {query}
        
        Determine:
        1. What is the user asking for?
        2. What data sources are needed?
        3. What operations are required (count, aggregate, filter, etc.)?
        4. What is the complexity level?
        5. What entities/collections are mentioned?
        """
        
        try:
            # Use AI provider manager if available for resilient AI calls
            if app_state.get("ai_provider_manager"):
                result = await app_state["ai_provider_manager"].generate_response(
                    understanding_task,
                    {"query": query},
                    cache_key=cache_key
                )
            else:
                # Fallback to agent runtime
                result = await planner.execute_with_reasoning(understanding_task, {"query": query})
            
            # Parse AI response into structured understanding
            output = result.get("output", "")
            if isinstance(output, dict):
                # Agent returned structured data, use it directly
                parsed_result = output
            else:
                parsed_result = self._parse_understanding(str(output))
            
            # Cache successful result
            if app_state.get("ai_cache"):
                await app_state["ai_cache"].set(cache_key, parsed_result, ttl=3600)  # 1 hour
            
            return parsed_result
            
        except Exception as e:
            logger.error(f"AI query understanding failed: {e}")
            fallback_result = {
                "intent": "data_query",
                "confidence": 0.3,
                "entities": [],
                "complexity": "simple",
                "error": str(e)
            }
            
            # Cache fallback result briefly
            if app_state.get("ai_cache"):
                await app_state["ai_cache"].set(cache_key, fallback_result, ttl=60)  # 1 minute
            
            return fallback_result
    
    async def _decide_approach(self, query: str, understanding: Dict[str, Any]) -> Dict[str, Any]:
        """Let AI decide the best execution approach"""
        
        # Check for explicit streaming requests first
        query_lower = query.lower()
        logger.info(f"Analyzing query for streaming: '{query}' -> '{query_lower}'")
        
        streaming_keywords = ["stream all", "export all", "download all", "fetch all"]
        for keyword in streaming_keywords:
            if keyword in query_lower:
                logger.info(f"âœ… Detected streaming request with keyword '{keyword}': '{query}'")
                return {
                    "workflow": "streaming",
                    "steps": ["prepare_streaming_response"],
                    "tool": None,
                    "params": {"query": query}
                }
        
        logger.info(f"âŒ No streaming keywords found in: '{query_lower}'")
        
        # Use agents to decide approach for other queries
        planner = self.agent_runtime.get_agent("query_planner")
        
        decision_task = f"""
        Based on this query understanding, decide the best approach:
        Query: {query}
        Understanding: {json.dumps(understanding, indent=2)}
        
        Choose approach:
        1. Simple tool execution (for basic operations)
        2. Complex aggregation workflow (for analytics)
        3. Multi-step reasoning (for complex questions)
        4. Streaming (for large data exports - use for queries containing 'stream', 'export all', 'download all')
        
        IMPORTANT: If the query asks to "stream", "export all", or "download all" data, always choose option 4 (Streaming).
        
        Provide specific steps needed.
        """
        
        try:
            # Use AI provider manager for resilient approach decisions
            cache_key = f"approach_decision:{hashlib.md5((query + str(understanding)).encode()).hexdigest()}"
            
            if app_state.get("ai_provider_manager"):
                result = await app_state["ai_provider_manager"].generate_response(
                    decision_task,
                    understanding,
                    cache_key=cache_key
                )
            else:
                # Fallback to agent runtime
                result = await planner.execute_with_reasoning(decision_task, understanding)
            
            # Parse approach
            output = result.get("output", "")
            if isinstance(output, dict):
                # Agent returned structured data, use it directly
                return output
                
        except Exception as e:
            logger.error(f"AI approach decision failed: {e}")
            # Fallback to simple approach for any query
            return {
                "type": "simple",
                "complexity": "low",
                "steps": ["execute_query"],
                "fallback": True,
                "error": str(e)
            }
        return self._parse_approach(str(output), query)
    
    async def _execute_with_ai(self, request_id: str, query: str, approach: Dict[str, Any], understanding: Dict[str, Any]) -> Dict[str, Any]:
        """Execute query using AI-driven approach"""
        workflow_type = approach.get("workflow", "complex_aggregation")
        
        # Update state
        await self.state_manager.update_state(request_id, {
            "status": "executing",
            "workflow": workflow_type
        })
        
        # Prepare context for workflow
        workflow_context = {
            "request_id": request_id,
            "query": query,
            "understanding": understanding,
            "approach": approach
        }
        
        # Execute appropriate workflow
        if workflow_type == "simple_execution":
            result = await self._execute_simple(query, approach)
        elif workflow_type == "streaming":
            result = await self._prepare_streaming(request_id, query, approach)
        else:
            # Use workflow engine for complex queries
            result = await app_state["workflow_engine"].execute_workflow(
                workflow_type,
                workflow_context
            )
        
        return result
    
    async def _execute_simple(self, query: str, approach: Dict[str, Any]) -> Dict[str, Any]:
        """Execute simple queries directly"""
        tool_name = approach.get("tool")
        params = approach.get("params", {})
        
        if not tool_name:
            # Let execution agent handle it
            executor = self.agent_runtime.get_agent("executor")
            result = await executor.execute_with_reasoning(query, {"approach": approach})
            return result
        
        # Direct tool execution
        import httpx
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{config.MCP_SERVER_URL}/execute",
                json={"tool": tool_name, "params": params},
                timeout=30.0
            )
            if response.status_code == 200:
                return response.json().get("result", {})
            else:
                raise Exception(f"Tool execution failed: {response.text}")
    
    async def _prepare_streaming(self, request_id: str, query: str, approach: Dict[str, Any]) -> Dict[str, Any]:
        """Prepare streaming response"""
        return {
            "type": "streaming",
            "stream_url": f"/api/stream/{request_id}",
            "message": "Large dataset detected. Use the streaming endpoint to retrieve results efficiently."
        }
    
    async def _generate_production_response(self, query: str, result: dict) -> str:
        """Generate intelligent, conversational responses using AI and real data - like ChatGPT/Claude!"""
        try:
            logger.info(f"ðŸ§  Generating intelligent response for: '{query[:100]}'")
            
            # Step 1: Get ACTUAL data from MongoDB collections
            real_data = await self._fetch_real_data_for_query(query)
            logger.info(f"ðŸ“Š Retrieved real data: {len(real_data.get('documents', []))} documents")
            
            # Step 2: Use AI to analyze the real data and generate intelligent response
            ai_response = await self._generate_ai_powered_response(query, real_data)
            
            return ai_response
            
        except Exception as e:
            logger.error(f"âŒ Error in intelligent response generation: {e}")
            return await self._generate_contextual_fallback(query, result)

    async def _fetch_real_data_for_query(self, query: str) -> dict:
        """Intelligently fetch data with smart caching system"""
        try:
            logger.info(f"ðŸ” Analyzing query for data needs: '{query[:100]}'")
            
            # Step 1: Check if we have relevant cached data analysis
            cached_analysis = await self._check_intelligent_cache(query)
            if cached_analysis:
                logger.info(f"ðŸŽ¯ Found relevant cached analysis for similar query")
                
                # Step 2: Determine if cached data fully answers the query
                cache_relevance = await self._assess_cache_relevance(query, cached_analysis)
                
                if cache_relevance["fully_covers"]:
                    logger.info("âœ… Cache fully covers the query - returning cached analysis")
                    return cached_analysis["data"]
                
                elif cache_relevance["partially_covers"]:
                    logger.info("ðŸ”„ Cache partially covers query - fetching additional data")
                    # Get additional data needed
                    additional_data = await self._fetch_additional_data(query, cached_analysis, cache_relevance["missing_aspects"])
                    # Combine cached + new data intelligently
                    combined_data = await self._combine_cached_and_fresh_data(cached_analysis["data"], additional_data, query)
                    # Cache the new combined analysis
                    await self._cache_data_analysis(query, combined_data)
                    return combined_data
            
            # Step 3: No relevant cache - use AI-driven dynamic data fetching
            logger.info("ðŸ†• No relevant cache found - using AI-driven dynamic analysis")
            fresh_data = await self._ai_powered_data_analysis(query)
            
            # Step 4: Cache the new analysis for future use
            await self._cache_data_analysis(query, fresh_data)
            
            return fresh_data
            
        except Exception as e:
            logger.error(f"âŒ Error in intelligent data fetching: {e}")
            return {
                "query_context": query,
                "collections_analyzed": [],
                "data_summary": {},
                "documents": [],
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def _check_intelligent_cache(self, query: str) -> dict:
        """Check if we have relevant cached data analysis with TTL management and metrics"""
        try:
            # Clean up expired cache first
            await self._cleanup_expired_cache()
            
            # Get all cached analysis keys
            cache_keys = await self._get_cache_analysis_keys()
            
            if not cache_keys:
                await self._record_cache_metrics("miss", "no_keys")
                return None
            
            # Use AI to find semantically similar cached analyses
            similar_cache = await self._find_semantically_similar_cache(query, cache_keys)
            
            if similar_cache:
                cached_data = await app_state["ai_cache"].get(similar_cache["cache_key"])
                if cached_data:
                    # Check cache age and validity
                    cache_age_hours = (datetime.now() - datetime.fromisoformat(similar_cache.get("timestamp", "1970-01-01"))).total_seconds() / 3600
                    max_age = 1 if any(word in query.lower() for word in ["real-time", "current", "latest", "now"]) else 24
                    
                    if cache_age_hours > max_age:
                        await app_state["ai_cache"].delete(similar_cache["cache_key"])
                        await self._record_cache_metrics("miss", "expired")
                        return None
                    
                    # Update cache usage metrics
                    await self._update_cache_usage(similar_cache["cache_key"])
                    await self._record_cache_metrics("hit", f"similarity_{similar_cache['similarity']:.2f}")
                    
                    return {
                        "cache_key": similar_cache["cache_key"],
                        "similarity_score": similar_cache["similarity"],
                        "original_query": similar_cache["original_query"],
                        "data": cached_data,
                        "timestamp": similar_cache["timestamp"],
                        "cache_age_hours": cache_age_hours
                    }
            
            await self._record_cache_metrics("miss", "no_similarity")
            return None
            
        except Exception as e:
            logger.error(f"Error checking intelligent cache: {e}")
            await self._record_cache_metrics("error", str(e))
            return None

    async def _cleanup_expired_cache(self):
        """Remove expired cache entries"""
        try:
            # Get all cache keys
            cache_keys = await self.redis.keys("conv_cache:*")
            expired_count = 0
            
            for key in cache_keys:
                cached_data = await self.redis.get(key)
                if cached_data:
                    try:
                        cache_obj = json.loads(cached_data)
                        timestamp = cache_obj.get("timestamp", "1970-01-01")
                        cache_age_hours = (datetime.now() - datetime.fromisoformat(timestamp)).total_seconds() / 3600
                        
                        # Different TTL for different query types
                        if "real-time" in cache_obj.get("query", "").lower() or "current" in cache_obj.get("query", "").lower():
                            max_age = 1  # 1 hour for real-time queries
                        else:
                            max_age = 24  # 24 hours for general queries
                        
                        if cache_age_hours > max_age:
                            await self.redis.delete(key)
                            expired_count += 1
                    except Exception as e:
                        logger.warning(f"Error processing cache key {key}: {e}")
                        await self.redis.delete(key)  # Remove corrupted entries
                        expired_count += 1
            
            if expired_count > 0:
                logger.info(f"ðŸ§¹ Cleaned up {expired_count} expired cache entries")
                
        except Exception as e:
            logger.error(f"Error cleaning expired cache: {e}")

    async def _record_cache_metrics(self, result_type: str, details: str):
        """Record cache performance metrics"""
        try:
            metrics_key = "cache_metrics"
            current_metrics = await self.redis.get(metrics_key)
            
            if current_metrics:
                metrics = json.loads(current_metrics)
            else:
                metrics = {
                    "hits": 0,
                    "misses": 0,
                    "errors": 0,
                    "hit_rate": 0.0,
                    "last_updated": datetime.now().isoformat(),
                    "details": {}
                }
            
            # Update counters
            if result_type == "hit":
                metrics["hits"] += 1
            elif result_type == "miss":
                metrics["misses"] += 1
            elif result_type == "error":
                metrics["errors"] += 1
            
            # Calculate hit rate
            total_requests = metrics["hits"] + metrics["misses"]
            if total_requests > 0:
                metrics["hit_rate"] = metrics["hits"] / total_requests
            
            # Track details
            if details not in metrics["details"]:
                metrics["details"][details] = 0
            metrics["details"][details] += 1
            
            metrics["last_updated"] = datetime.now().isoformat()
            
            # Store with 7 day TTL
            await self.redis.setex(metrics_key, 604800, json.dumps(metrics))
            
        except Exception as e:
            logger.error(f"Error recording cache metrics: {e}")

    async def _update_cache_usage(self, cache_key: str):
        """Update cache usage statistics"""
        try:
            cached_data = await self.redis.get(cache_key)
            if cached_data:
                cache_obj = json.loads(cached_data)
                cache_obj["usage_count"] = cache_obj.get("usage_count", 0) + 1
                cache_obj["last_accessed"] = datetime.now().isoformat()
                
                # Update with extended TTL on access
                await self.redis.setex(cache_key, 86400, json.dumps(cache_obj))
                
        except Exception as e:
            logger.error(f"Error updating cache usage: {e}")

    async def _assess_cache_relevance(self, query: str, cached_analysis: dict) -> dict:
        """Use AI to determine how well cached data covers the new query"""
        try:
            ai_prompt = f"""
            CURRENT QUERY: "{query}"
            
            CACHED ANALYSIS DETAILS:
            - Original Query: "{cached_analysis['original_query']}"
            - Data Collections: {cached_analysis['data'].get('collections_used', [])}
            - Analysis Type: {cached_analysis['data'].get('analysis_type', 'general')}
            - Document Count: {cached_analysis['data'].get('total_found', 0)}
            
            Analyze if the cached data can answer the current query:
            
            1. FULLY_COVERS: Can cached data completely answer the new query? (true/false)
            2. PARTIALLY_COVERS: Does cached data provide some relevant information? (true/false)  
            3. MISSING_ASPECTS: What specific data aspects are missing for the new query? (list)
            4. CONFIDENCE: How confident are you in this assessment? (0.0-1.0)
            
            Respond in JSON format:
            {{
                "fully_covers": boolean,
                "partially_covers": boolean,
                "missing_aspects": ["aspect1", "aspect2"],
                "confidence": 0.0-1.0,
                "reasoning": "explanation"
            }}
            """
            
            response = await self._call_ai_provider(ai_prompt)
            import json
            relevance_assessment = json.loads(response)
            
            logger.info(f"ðŸ§  Cache relevance assessment: {relevance_assessment['confidence']:.2f} confidence")
            return relevance_assessment
            
        except Exception as e:
            logger.error(f"Error assessing cache relevance: {e}")
            return {"fully_covers": False, "partially_covers": False, "missing_aspects": [], "confidence": 0.0}

    async def _fetch_additional_data(self, query: str, cached_analysis: dict, missing_aspects: list) -> dict:
        """Fetch only the missing data aspects needed to complete the analysis"""
        try:
            logger.info(f"ðŸ” Fetching additional data for aspects: {missing_aspects}")
            
            additional_data = {
                "documents": [],
                "collections_used": [],
                "total_found": 0,
                "analysis_type": "supplementary",
                "aspects_covered": missing_aspects
            }
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                
                for aspect in missing_aspects:
                    aspect_lower = aspect.lower()
                    
                    if "customer" in aspect_lower or "user" in aspect_lower:
                        # Fetch detailed user behavior data
                        response = await client.post(
                            f"{config.MCP_SERVER_URL}/execute",
                            json={
                                "tool": "mongodb_aggregate",
                                "params": {
                                    "collection": "user_activity",
                                    "pipeline": [
                                        {"$group": {"_id": "$user_id", "events": {"$sum": 1}, "latest_activity": {"$max": "$timestamp"}}},
                                        {"$sort": {"events": -1}},
                                        {"$limit": 20}
                                    ]
                                }
                            }
                        )
                        if response.status_code == 200:
                            result_data = response.json()
                            if result_data.get("success"):
                                additional_data["documents"].extend(result_data["result"]["documents"])
                                additional_data["collections_used"].append("user_activity")
                    
                    elif "campaign" in aspect_lower or "marketing" in aspect_lower:
                        # Fetch marketing performance data
                        response = await client.post(
                            f"{config.MCP_SERVER_URL}/execute",
                            json={
                                "tool": "mongodb_find",
                                "params": {
                                    "collection": "marketing_campaigns",
                                    "limit": 15
                                }
                            }
                        )
                        if response.status_code == 200:
                            result_data = response.json()
                            if result_data.get("success"):
                                additional_data["documents"].extend(result_data["result"]["documents"])
                                additional_data["collections_used"].append("marketing_campaigns")
                    
                    elif "content" in aspect_lower or "engagement" in aspect_lower:
                        # Fetch content engagement data
                        response = await client.post(
                            f"{config.MCP_SERVER_URL}/execute",
                            json={
                                "tool": "mongodb_aggregate",
                                "params": {
                                    "collection": "user_activity",
                                    "pipeline": [
                                        {"$group": {"_id": "$event_data.content_id", "views": {"$sum": 1}, "unique_users": {"$addToSet": "$user_id"}}},
                                        {"$sort": {"views": -1}},
                                        {"$limit": 15}
                                    ]
                                }
                            }
                        )
                        if response.status_code == 200:
                            result_data = response.json()
                            if result_data.get("success"):
                                additional_data["documents"].extend(result_data["result"]["documents"])
                                additional_data["collections_used"].append("user_activity")
                
                additional_data["total_found"] = len(additional_data["documents"])
                
            logger.info(f"âœ… Fetched {additional_data['total_found']} additional documents")
            return additional_data
            
        except Exception as e:
            logger.error(f"Error fetching additional data: {e}")
            return {"documents": [], "collections_used": [], "total_found": 0, "error": str(e)}

    async def _combine_cached_and_fresh_data(self, cached_data: dict, fresh_data: dict, query: str) -> dict:
        """Intelligently combine cached and fresh data for comprehensive analysis"""
        try:
            combined_data = {
                "query_context": query,
                "documents": [],
                "collections_used": list(set(cached_data.get("collections_used", []) + fresh_data.get("collections_used", []))),
                "total_found": 0,
                "analysis_type": "combined_analysis",
                "data_sources": {
                    "cached": {
                        "count": cached_data.get("total_found", 0),
                        "collections": cached_data.get("collections_used", [])
                    },
                    "fresh": {
                        "count": fresh_data.get("total_found", 0),
                        "collections": fresh_data.get("collections_used", []),
                        "aspects": fresh_data.get("aspects_covered", [])
                    }
                },
                "timestamp": datetime.now().isoformat()
            }
            
            # Combine documents intelligently (avoid duplicates)
            cached_docs = cached_data.get("documents", [])
            fresh_docs = fresh_data.get("documents", [])
            
            # Use document IDs to deduplicate
            seen_ids = set()
            
            for doc in cached_docs + fresh_docs:
                doc_id = doc.get("_id") or doc.get("id") or str(doc)[:100]
                if doc_id not in seen_ids:
                    combined_data["documents"].append(doc)
                    seen_ids.add(doc_id)
            
            combined_data["total_found"] = len(combined_data["documents"])
            
            logger.info(f"ðŸ”— Combined data: {len(cached_docs)} cached + {len(fresh_docs)} fresh = {combined_data['total_found']} unique documents")
            return combined_data
            
        except Exception as e:
            logger.error(f"Error combining cached and fresh data: {e}")
            return cached_data  # Fallback to cached data

    async def _ai_powered_data_analysis(self, query: str) -> dict:
        """ðŸ¤– Completely dynamic AI-powered analysis with real-time database connection - NO CACHING"""
        logger.info(f"ðŸ¤– Starting FRESH AI analysis for: '{query[:50]}...'")
        
        try:
            # STEP 1: Understand user intent semantically (no caching)
            logger.info("ðŸ§  AI analyzing user intent...")
            user_intent = self._understand_user_intent_deeply(query)
            logger.info(f"ðŸŽ¯ User Intent: {user_intent['primary_intent']} | Confidence: {user_intent['confidence']}")
            
            # STEP 2: Fresh database discovery every time (no caching)
            logger.info("ðŸ” Connecting to LIVE database for fresh discovery...")
            collections_discovery = await self._discover_live_database()
            
            if not collections_discovery.get("success"):
                return {
                    "error": f"Could not connect to database: {collections_discovery.get('error')}",
                    "user_intent": user_intent
                }
            
            logger.info(f"âœ… Connected to {collections_discovery['database']} with {len(collections_discovery['collections'])} collections")
            
            # STEP 3: AI creates dynamic strategy based on intent + live data
            logger.info("ðŸ§  AI creating dynamic strategy...")
            strategy = await self._create_ai_strategy(user_intent, collections_discovery)
            logger.info(f"ï¿½ Strategy: {strategy['analysis_type']} targeting {len(strategy.get('target_collections', []))} collections")
            
            # STEP 4: Execute real-time data fetching
            logger.info("ðŸ”„ Fetching LIVE data...")
            live_data = await self._fetch_live_data(strategy, collections_discovery)
            logger.info(f"ðŸ“Š Fetched {live_data['total_records']} records from {len(live_data['collections_data'])} collections")
            
            # STEP 5: Generate dynamic AI response (no caching)
            logger.info("ðŸ§  Generating fresh AI response...")
            ai_response = await self._generate_dynamic_response(query, user_intent, strategy, live_data)
            
            return {
                "success": True,
                "user_intent": user_intent,
                "strategy": strategy,
                "live_data_summary": {
                    "total_records": live_data['total_records'],
                    "collections_analyzed": list(live_data['collections_data'].keys()),
                    "database": collections_discovery['database']
                },
                "ai_response": ai_response,
                "timestamp": datetime.utcnow().isoformat(),
                "is_fresh_analysis": True  # Always fresh, never cached
            }
            
        except Exception as e:
            logger.error(f"Error in fresh AI analysis: {e}")
            return {
                "error": str(e),
                "query": query,
                "timestamp": datetime.utcnow().isoformat()
            }

    def _understand_user_intent_deeply(self, query: str) -> dict:
        """ðŸ§  Deep semantic understanding of user intent - no caching, fresh analysis every time"""
        query_lower = query.lower().strip()
        
        # Advanced semantic intent mapping
        intent_patterns = {
            "database_structure": [
                "database structure", "what collections", "what data", "database schema",
                "collections do i have", "database overview", "show me collections",
                "database contents", "what's in database", "structure of database"
            ],
            "user_analysis": [
                "user behavior", "user activity", "customer analysis", "user patterns",
                "user insights", "customer insights", "user demographics", "user engagement"
            ],
            "business_analysis": [
                "sales analysis", "revenue", "business performance", "orders", "transactions",
                "business insights", "profit analysis", "sales trends", "business metrics"
            ],
            "content_analysis": [
                "content analysis", "digital content", "content performance", "media analysis",
                "content metrics", "content insights", "content trends"
            ],
            "marketing_analysis": [
                "marketing", "campaigns", "marketing performance", "campaign analysis",
                "marketing insights", "advertising", "marketing roi"
            ],
            "operational_analysis": [
                "operations", "warehouse", "shipping", "logistics", "inventory",
                "operational efficiency", "supply chain"
            ]
        }
        
        # Detect primary intent
        detected_intents = []
        confidence_scores = {}
        
        for intent_type, patterns in intent_patterns.items():
            matches = sum(1 for pattern in patterns if pattern in query_lower)
            if matches > 0:
                confidence = min(matches * 0.3, 1.0)  # Max confidence 1.0
                detected_intents.append(intent_type)
                confidence_scores[intent_type] = confidence
        
        # Determine primary intent
        if detected_intents:
            primary_intent = max(detected_intents, key=lambda x: confidence_scores[x])
            confidence = confidence_scores[primary_intent]
        else:
            primary_intent = "general_analysis"
            confidence = 0.5
        
        # Analyze query complexity and data needs
        data_scope = "comprehensive" if any(word in query_lower for word in ["all", "complete", "full", "comprehensive", "detailed"]) else "focused"
        response_style = "technical" if any(word in query_lower for word in ["schema", "structure", "technical", "details"]) else "business"
        
        return {
            "primary_intent": primary_intent,
            "all_detected_intents": detected_intents,
            "confidence": confidence,
            "data_scope": data_scope,
            "response_style": response_style,
            "original_query": query,
            "semantic_keywords": [word for word in query_lower.split() if len(word) > 3]
        }

    async def _discover_live_database(self) -> dict:
        """ðŸ” Connect to LIVE database from .env and discover collections - NO CACHING"""
        try:
            logger.info("ðŸ”— Connecting to live MongoDB database from .env...")
            
            # Use MCP server to get real collections from actual database
            response = await self._safe_http_call(
                "mongodb_tool", 
                {"action": "list_collections"}
            )
            
            if not response or "result" not in response:
                return {
                    "success": False,
                    "error": "Could not connect to live database",
                    "collections": []
                }
            
            db_info = response["result"]
            collections_list = db_info.get("collections", [])
            
            # Extract collection names and get live data
            collections = []
            total_records = 0
            
            for col_info in collections_list:
                if isinstance(col_info, dict) and "name" in col_info:
                    collection_name = col_info["name"]
                    
                    # Get live count for each collection
                    try:
                        count_response = await self._safe_http_call(
                            "mongodb_tool",
                            {"action": "count_documents", "collection": collection_name}
                        )
                        
                        count = count_response.get("result", 0) if count_response else 0
                        total_records += count
                        
                        collections.append({
                            "name": collection_name,
                            "count": count,
                            "size_category": "large" if count > 10000 else "medium" if count > 1000 else "small"
                        })
                        
                        logger.info(f"  ðŸ“Š {collection_name}: {count:,} records")
                        
                    except Exception as e:
                        logger.warning(f"Could not get count for {collection_name}: {e}")
                        collections.append({
                            "name": collection_name,
                            "count": 0,
                            "size_category": "unknown"
                        })
            
            logger.info(f"âœ… Live database discovery complete: {len(collections)} collections, {total_records:,} total records")
            
            return {
                "success": True,
                "database": db_info.get("database", "Unknown"),
                "collections": collections,
                "total_collections": len(collections),
                "total_records": total_records,
                "discovery_timestamp": datetime.utcnow().isoformat(),
                "is_live_data": True
            }
            
        except Exception as e:
            logger.error(f"Error discovering live database: {e}")
            return {
                "success": False,
                "error": str(e),
                "collections": []
            }

    async def _create_ai_strategy(self, user_intent: dict, collections_discovery: dict) -> dict:
        """ðŸ§  AI creates dynamic strategy based on user intent and live database state"""
        try:
            primary_intent = user_intent["primary_intent"]
            available_collections = collections_discovery["collections"]
            total_records = collections_discovery["total_records"]
            
            logger.info(f"ðŸ§  Creating strategy for intent '{primary_intent}' with {len(available_collections)} collections")
            
            # Intent-based collection targeting
            if primary_intent == "database_structure":
                target_collections = [col["name"] for col in available_collections]
                analysis_type = "comprehensive_database_overview"
                sample_size = 3  # Small samples for structure overview
                
            elif primary_intent == "user_analysis":
                target_collections = [col["name"] for col in available_collections 
                                    if any(keyword in col["name"].lower() for keyword in ["user", "customer", "activity"])]
                analysis_type = "user_behavioral_analysis"
                sample_size = 20  # More samples for behavior analysis
                
            elif primary_intent == "business_analysis":
                target_collections = [col["name"] for col in available_collections 
                                    if any(keyword in col["name"].lower() for keyword in ["order", "sale", "transaction", "revenue"])]
                analysis_type = "business_performance_analysis"
                sample_size = 15
                
            elif primary_intent == "marketing_analysis":
                target_collections = [col["name"] for col in available_collections 
                                    if any(keyword in col["name"].lower() for keyword in ["marketing", "campaign", "ad"])]
                analysis_type = "marketing_effectiveness_analysis"
                sample_size = 10
                
            else:
                # General analysis - select most data-rich collections
                sorted_collections = sorted(available_collections, key=lambda x: x["count"], reverse=True)
                target_collections = [col["name"] for col in sorted_collections[:5]]
                analysis_type = "general_data_analysis"
                sample_size = 10
            
            # Adjust sample size based on data scope
            if user_intent["data_scope"] == "comprehensive":
                sample_size = min(sample_size * 2, 50)
            
            return {
                "analysis_type": analysis_type,
                "target_collections": target_collections,
                "sample_size": sample_size,
                "user_intent": user_intent,
                "total_available_records": total_records,
                "strategy_timestamp": datetime.utcnow().isoformat(),
                "is_dynamic": True
            }
            
        except Exception as e:
            logger.error(f"Error creating AI strategy: {e}")
            return {
                "analysis_type": "fallback_analysis",
                "target_collections": [],
                "sample_size": 5,
                "error": str(e)
            }

    async def _fetch_live_data(self, strategy: dict, collections_discovery: dict) -> dict:
        """ðŸ”„ Fetch live data from database based on AI strategy - NO CACHING"""
        try:
            target_collections = strategy["target_collections"]
            sample_size = strategy["sample_size"]
            
            logger.info(f"ðŸ”„ Fetching live data from {len(target_collections)} collections")
            
            collections_data = {}
            total_records = 0
            
            for collection_name in target_collections:
                try:
                    # Get live sample data
                    sample_response = await self._safe_http_call(
                        "mongodb_tool",
                        {"action": "query_data", "collection": collection_name, "pipeline": [{"$sample": {"size": sample_size}}]}
                    )
                    
                    if sample_response and "result" in sample_response:
                        samples = sample_response["result"]
                        if isinstance(samples, list):
                            collections_data[collection_name] = {
                                "samples": samples,
                                "sample_count": len(samples),
                                "fields": list(samples[0].keys()) if samples else [],
                                "collection_info": next((col for col in collections_discovery["collections"] if col["name"] == collection_name), {})
                            }
                            total_records += len(samples)
                            logger.info(f"  âœ… {collection_name}: {len(samples)} live samples")
                        else:
                            logger.warning(f"  âŒ {collection_name}: Invalid data format")
                    else:
                        logger.warning(f"  âŒ {collection_name}: No response")
                        
                except Exception as e:
                    logger.error(f"Error fetching from {collection_name}: {e}")
            
            return {
                "collections_data": collections_data,
                "total_records": total_records,
                "strategy_used": strategy,
                "fetch_timestamp": datetime.utcnow().isoformat(),
                "is_live_data": True
            }
            
        except Exception as e:
            logger.error(f"Error fetching live data: {e}")
            return {
                "collections_data": {},
                "total_records": 0,
                "error": str(e)
            }

    async def _generate_dynamic_response(self, query: str, user_intent: dict, strategy: dict, live_data: dict) -> str:
        """ðŸ§  Generate intelligent, contextual AI response based on live data - NO CACHING"""
        try:
            primary_intent = user_intent["primary_intent"]
            collections_data = live_data["collections_data"]
            total_records = live_data["total_records"]
            
            # Build contextual response based on intent and actual data
            if primary_intent == "database_structure":
                response_parts = []
                response_parts.append(f"ðŸ—„ï¸ **Database Structure Analysis**")
                response_parts.append(f"Your database contains {len(collections_data)} collections with {total_records:,} total sample records:")
                response_parts.append("")
                
                for collection_name, data in collections_data.items():
                    collection_info = data["collection_info"]
                    total_count = collection_info.get("count", 0)
                    fields = data["fields"]
                    
                    response_parts.append(f"ðŸ“Š **{collection_name}**")
                    response_parts.append(f"   â€¢ Records: {total_count:,}")
                    response_parts.append(f"   â€¢ Fields: {', '.join(fields[:5])}{'...' if len(fields) > 5 else ''}")
                    response_parts.append(f"   â€¢ Category: {collection_info.get('size_category', 'unknown')}")
                    response_parts.append("")
                
                return "\n".join(response_parts)
                
            elif primary_intent == "user_analysis":
                user_collections = [name for name in collections_data.keys() if "user" in name.lower()]
                response_parts = []
                response_parts.append(f"ðŸ‘¥ **User Analysis**")
                response_parts.append(f"Found {len(user_collections)} user-related collections with live data:")
                
                for collection_name in user_collections:
                    data = collections_data[collection_name]
                    total_count = data["collection_info"].get("count", 0)
                    response_parts.append(f"â€¢ {collection_name}: {total_count:,} records")
                
                return "\n".join(response_parts)
                
            else:
                # General intelligent response
                collection_summary = []
                for name, data in collections_data.items():
                    count = data["collection_info"].get("count", 0)
                    collection_summary.append(f"{name} ({count:,} records)")
                
                return f"ðŸ“ˆ **Analysis Complete**\n\nAnalyzed {len(collections_data)} collections: {', '.join(collection_summary)}\n\nTotal sample data: {total_records:,} records\nAnalysis type: {strategy['analysis_type']}"
            
        except Exception as e:
            logger.error(f"Error generating dynamic response: {e}")
            return f"Analysis completed with {len(live_data.get('collections_data', {}))} collections and {live_data.get('total_records', 0)} records."

# === API ENDPOINTS ===

@app.get("/")
async def root():
    return {
        "service": "Hydrogen AI Orchestrator",
        "version": "3.0.0",
        "status": "running",
        "intelligence": "AI-powered",
        "endpoints": {
            "query": "/api/query",
            "status": "/api/status/{request_id}",
            "result": "/api/result/{request_id}",
            "stream": "/api/stream/{request_id}",
            "health": "/health"
        }
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "service": "Hydrogen AI Orchestrator",
        "version": "3.0.0"
    }

@app.post("/api/query")
async def query_data(request: QueryRequest) -> QueryResponse:
    """COMPLETELY DYNAMIC AI QUERY PROCESSING - NO CACHING - FRESH EVERY TIME"""
    request_id = str(uuid.uuid4())
    logger.info(f"ðŸš€ FRESH DYNAMIC QUERY [{request_id[:8]}]: {request.query[:100]}")
    
    try:
        # STEP 1: Initialize processor for this request (no cache)
        processor = QueryProcessor()
        
        # STEP 2: Process with our NEW DYNAMIC SYSTEM (no caching)
        logger.info("ðŸ”¥ Using COMPLETELY DYNAMIC system - no cache, fresh analysis")
        result = await processor._ai_powered_data_analysis(request.query)
        
        # STEP 3: Generate final response
        if result.get("success"):
            human_response = result["ai_response"]
            logger.info(f"âœ… FRESH ANALYSIS COMPLETE: {len(human_response)} chars")
        else:
            human_response = f"Analysis error: {result.get('error', 'Unknown error')}"
            logger.error(f"âŒ Analysis failed: {result.get('error')}")
        
        return QueryResponse(
            request_id=request_id,
            status="completed",
            understanding={"steps": [], "estimated_time": 0, "resources_needed": [], "optimization_hints": []},
            approach={"steps": [], "estimated_time": 0, "resources_needed": [], "optimization_hints": []},
            human_response=human_response,
            result={
                "success": result.get("success", False),
                "results": result.get("live_data_summary", {}),
                "messages": [],
                "metrics": ""
            },
            from_cache=False  # NEVER from cache - always fresh
        )
        
    except Exception as e:
        logger.error(f"âŒ CRITICAL ERROR in fresh dynamic query: {e}")
        return QueryResponse(
            request_id=request_id,
            status="error",
            understanding={"steps": [], "estimated_time": 0, "resources_needed": [], "optimization_hints": []},
            approach={"steps": [], "estimated_time": 0, "resources_needed": [], "optimization_hints": []},
            human_response=f"System error: {str(e)}",
            result={"success": False, "results": {}, "messages": [], "metrics": ""},
            from_cache=False
        )

@app.get("/api/status/{request_id}")
async def get_query_status(request_id: str):
    return {"request_id": request_id, "status": "completed", "progress": 100}

@app.get("/api/result/{request_id}")
async def get_query_result(request_id: str):
    return {"request_id": request_id, "status": "completed", "result": "No stored results - using real-time processing"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
                "performance_analysis": ["performance", "metrics", "kpi", "analytics", "insights"],
                "data_overview": ["overview", "summary", "report", "show", "display", "list"]
            }
            
            # AI determines what user actually wants
            detected_intents = []
            for intent_type, keywords in intent_mapping.items():
                if any(keyword in query_lower for keyword in keywords):
                    detected_intents.append(intent_type)
            
            # Determine primary intent
            primary_intent = detected_intents[0] if detected_intents else "general_analysis"
            
            return {
                "intent_type": primary_intent,
                "detected_intents": detected_intents,
                "requires_collections": self._get_relevant_collections_for_intent(primary_intent),
                "analysis_depth": "deep" if any(word in query_lower for word in ["deep", "detailed", "comprehensive", "analyze"]) else "standard"
            }
            
        except Exception as e:
            logger.error(f"Error understanding user intent: {e}")
            return {"intent_type": "general_analysis", "detected_intents": [], "requires_collections": [], "analysis_depth": "standard"}

    def _get_relevant_collections_for_intent(self, intent_type: str) -> list:
        """ðŸŽ¯ Map user intent to relevant database collections"""
        intent_to_collections = {
            "database_structure": [],  # All collections
            "user_analysis": ["users", "user_activity", "customers"],
            "content_analysis": ["digital_content", "user_activity", "content"],
            "business_analysis": ["orders", "revenue", "sales", "transactions"],
            "marketing_analysis": ["marketing_campaigns", "user_activity"],
            "performance_analysis": ["user_activity", "orders", "marketing_campaigns"]
        }
        return intent_to_collections.get(intent_type, [])

    def _analyze_field_types(self, samples: list) -> dict:
        """ðŸ”¬ AI analyzes field types and patterns in sample data"""
        field_types = {}
        if not samples:
            return field_types
            
        for sample in samples:
            for field, value in sample.items():
                if field not in field_types:
                    field_types[field] = {"type": type(value).__name__, "examples": []}
                if len(field_types[field]["examples"]) < 3:
                    field_types[field]["examples"].append(value)
        
        return field_types

    def _infer_collection_purpose(self, collection_name: str, samples: list) -> str:
        """ðŸ§  AI infers the purpose of a collection based on name and structure"""
        name_lower = collection_name.lower()
        
        if "user" in name_lower:
            return "User data and profiles"
        elif "activity" in name_lower:
            return "User activity and behavior tracking"
        elif "order" in name_lower:
            return "Transaction and order data"
        elif "product" in name_lower:
            return "Product catalog and inventory"
        elif "marketing" in name_lower or "campaign" in name_lower:
            return "Marketing and campaign data"
        elif "content" in name_lower:
            return "Digital content and media"
        elif samples and any("timestamp" in str(sample).lower() or "date" in str(sample).lower() for sample in samples):
            return "Time-series or event data"
        else:
            return "General business data"

    async def _ai_discover_collections(self) -> dict:
        """ðŸ¤– AI discovers and analyzes database collections intelligently"""
        logger.info("ðŸ” AI discovering database collections...")
        
        try:
            # Get list of collections using MCP server
            collections_response = await self._safe_http_call(
                "mongodb_tool", 
                {"action": "list_collections"}
            )
            
            if not collections_response or "result" not in collections_response:
                logger.warning("Could not retrieve collections")
                return {"collections": [], "error": "Could not retrieve collections"}
            
            collections_data = collections_response["result"]
            collections_list = collections_data.get("collections", [])
            
            # Extract collection names from the response structure
            collections = [col["name"] for col in collections_list if isinstance(col, dict) and "name" in col]
            logger.info(f"ðŸ” Found {len(collections)} collections: {collections}")
            
            # AI analyzes each collection to understand structure and content
            collection_analysis = {}
            for collection in collections[:10]:  # Reasonable limit
                try:
                    logger.info(f"  ðŸ“Š Analyzing collection: {collection}")
                    
                    # Get sample documents + count using MCP server
                    sample_response = await self._safe_http_call(
                        "mongodb_tool",
                        {"action": "query_data", "collection": collection, "pipeline": [{"$limit": 3}]}
                    )
                    
                    logger.info(f"  ðŸ“Š Sample response for {collection}: {type(sample_response)}")
                    
                    count_response = await self._safe_http_call(
                        "mongodb_tool",
                        {"action": "count_documents", "collection": collection}
                    )
                    
                    logger.info(f"  ðŸ“Š Count response for {collection}: {type(count_response)}")
                    
                    if sample_response and count_response:
                        samples = sample_response.get("result", [])
                        count = count_response.get("result", 0)
                        
                        logger.info(f"  ðŸ“Š Samples type for {collection}: {type(samples)}, count: {count}")
                        
                        # Ensure samples is a list for slicing operations
                        if not isinstance(samples, list):
                            logger.warning(f"  âš ï¸  Samples for {collection} is not a list: {type(samples)}")
                            samples = []
                        
                        logger.info(f"  ðŸ“Š Processing collection analysis for {collection}")
                        
                        collection_analysis[collection] = {
                            "count": count,
                            "sample_fields": list(samples[0].keys()) if samples and isinstance(samples[0], dict) else [],
                            "sample_data": samples[:2] if samples else [],  # Keep 2 samples for AI analysis
                            "data_types": self._analyze_field_types(samples) if samples else {},
                            "purpose": self._infer_collection_purpose(collection, samples) if samples else "Unknown",
                            "relevance_score": 0  # Will be calculated by AI
                        }
                        
                        logger.info(f"  âœ“ {collection}: {count:,} documents, {len(samples)} samples")
                        
                except Exception as e:
                    logger.warning(f"Could not analyze collection {collection}: {e}")
                    collection_analysis[collection] = {"count": 0, "error": str(e)}
            
            return {
                "collections": collections,
                "analysis": collection_analysis,
                "total_collections": len(collections),
                "discovery_timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error in AI collection discovery: {e}")
            return {"collections": [], "error": str(e)}
            return {"collections": [], "error": str(e)}

    async def _ai_decide_data_strategy(self, query: str, collections_discovery: dict) -> dict:
        """ðŸ§  AI intelligently decides optimal data fetching strategy based on semantic understanding"""
        try:
            user_intent = collections_discovery.get("user_intent", {})
            available_collections = collections_discovery.get("available_collections", [])
            collection_analysis = collections_discovery.get("collection_analysis", {})
            
            logger.info(f"ðŸ§  AI creating strategy for intent: {user_intent.get('intent_type', 'unknown')}")
            
            # AI creates intelligent strategy based on user intent
            if user_intent.get("intent_type") == "database_structure":
                return {
                    "strategy_name": "comprehensive_database_discovery",
                    "analysis_type": "database_structure_analysis",
                    "primary_collections": available_collections,
                    "data_approach": "structure_and_samples",
                    "response_focus": "database_overview_with_structure"
                }
            
            elif user_intent.get("intent_type") == "user_analysis":
                relevant_collections = [c for c in available_collections if any(keyword in c.lower() for keyword in ["user", "customer", "activity"])]
                return {
                    "strategy_name": "deep_user_behavioral_analysis", 
                    "analysis_type": "user_behavior_analysis",
                    "primary_collections": relevant_collections[:3],
                    "data_approach": "aggregated_analysis",
                    "response_focus": "user_insights_and_patterns"
                }
            
            else:
                # Default intelligent strategy
                # Find collections with most data and most relevant to query
                query_lower = query.lower()
                scored_collections = []
                
                for collection, analysis in collection_analysis.items():
                    score = 0
                    # Score based on data volume
                    score += min(analysis.get("count", 0) / 1000, 10)  # Up to 10 points for volume
                    
                    # Score based on query relevance
                    for word in query_lower.split():
                        if word in collection.lower():
                            score += 5
                    
                    scored_collections.append((collection, score, analysis.get("count", 0)))
                
                # Sort by score and take top collections
                scored_collections.sort(key=lambda x: x[1], reverse=True)
                primary_collections = [item[0] for item in scored_collections[:4]]
                
                return {
                    "strategy_name": "intelligent_multi_collection_analysis",
                    "analysis_type": "contextual_data_analysis", 
                    "primary_collections": primary_collections,
                    "data_approach": "targeted_sampling",
                    "response_focus": "relevant_insights"
                }
            
        except Exception as e:
            logger.error(f"Error in AI strategy decision: {e}")
            return self._intelligent_fallback_strategy(query, collections_discovery)

    async def _ai_execute_data_fetching(self, strategy: Dict, query: str) -> Dict:
        """Execute data fetching based on AI strategy with semantic understanding"""
        logger.info("ðŸ”„ Executing AI-driven data fetching with semantic understanding...")
        
        try:
            # First, get actual collections from database
            collections_info = await self._ai_discover_collections()
            
            if not collections_info or not collections_info.get("collections"):
                logger.warning("No collections found during data fetching")
                return {"error": "No database collections available", "collections": []}
            
            available_collections = [col["name"] for col in collections_info["collections"]]
            logger.info(f"ðŸ“š Available collections: {available_collections}")
            
            # Map strategy collections to actual collections using semantic understanding
            primary_collections = []
            for strategy_col in strategy.get("primary_collections", []):
                # Try exact match first
                if strategy_col in available_collections:
                    primary_collections.append(strategy_col)
                else:
                    # Use semantic matching for collections
                    semantic_match = self._find_semantic_collection_match(strategy_col, available_collections)
                    if semantic_match:
                        primary_collections.append(semantic_match)
                        logger.info(f"ðŸ” Mapped '{strategy_col}' â†’ '{semantic_match}'")
            
            if not primary_collections:
                # If no specific collections mapped, use intelligent collection selection
                primary_collections = self._select_relevant_collections_by_intent(
                    query, available_collections, collections_info["collections"]
                )
            
            logger.info(f"ðŸŽ¯ Using collections: {primary_collections}")
            
            # Fetch data from selected collections
            fetched_data = {}
            for collection in primary_collections:
                try:
                    data = await self._fetch_collection_data(collection, strategy, query)
                    if data:
                        fetched_data[collection] = data
                        logger.info(f"âœ… Fetched {len(data)} records from {collection}")
                except Exception as e:
                    logger.error(f"Error fetching from {collection}: {e}")
            
            return {
                "strategy": strategy,
                "collections_used": primary_collections,
                "data": fetched_data,
                "total_records": sum(len(data) for data in fetched_data.values() if isinstance(data, list))
            }
            
        except Exception as e:
            logger.error(f"Error in AI data execution: {e}")
            return {"error": str(e), "collections": []}

    def _find_semantic_collection_match(self, target_collection: str, available_collections: List[str]) -> Optional[str]:
        """Find semantic matches between strategy collections and actual collections"""
        target_lower = target_collection.lower()
        
        # Semantic mapping rules
        semantic_mappings = {
            "user": ["users", "user_profiles", "customers"],
            "activity": ["user_activity", "activities", "events", "logs"],
            "product": ["products", "items", "catalog"],
            "order": ["orders", "purchases", "transactions"],
            "content": ["content", "posts", "articles", "media"],
            "review": ["reviews", "ratings", "feedback"],
            "analytics": ["analytics", "metrics", "stats"]
        }
        
        # Check semantic mappings
        for semantic_key, variants in semantic_mappings.items():
            if semantic_key in target_lower:
                for variant in variants:
                    for available in available_collections:
                        if variant in available.lower():
                            return available
        
        # Fuzzy matching for similar names
        for available in available_collections:
            if target_lower in available.lower() or available.lower() in target_lower:
                return available
        
        return None

    def _select_relevant_collections_by_intent(self, query: str, available_collections: List[str], collections_info: List[Dict]) -> List[str]:
        """Select relevant collections based on query intent and semantic understanding"""
        intent = self._understand_user_intent(query)
        query_lower = query.lower()
        
        relevant_collections = []
        
        # Intent-based collection selection
        if intent in ["user_analysis", "user_behavior"]:
            # Prioritize user-related collections
            for collection in available_collections:
                if any(keyword in collection.lower() for keyword in ["user", "customer", "profile", "activity"]):
                    relevant_collections.append(collection)
        
        elif intent in ["product_analysis", "catalog_analysis"]:
            # Prioritize product-related collections
            for collection in available_collections:
                if any(keyword in collection.lower() for keyword in ["product", "item", "catalog", "inventory"]):
                    relevant_collections.append(collection)
        
        elif intent in ["transaction_analysis", "sales_analysis"]:
            # Prioritize transaction-related collections
            for collection in available_collections:
                if any(keyword in collection.lower() for keyword in ["order", "transaction", "purchase", "sale"]):
                    relevant_collections.append(collection)
        
        elif intent == "database_structure":
            # For structure analysis, include all collections
            relevant_collections = available_collections[:5]  # Limit to avoid overwhelming
        
        # If no specific collections found, use collections with most data
        if not relevant_collections and collections_info:
            sorted_collections = sorted(collections_info, key=lambda x: x.get("count", 0), reverse=True)
            relevant_collections = [col["name"] for col in sorted_collections[:3]]
        
        return relevant_collections or available_collections[:3]

    async def _fetch_collection_data(self, collection: str, strategy: Dict, query: str) -> List[Dict]:
        """Fetch sample data from a collection with intelligent sampling"""
        try:
            # Prepare MCP call for data fetching
            mcp_request = {
                "action": "query_data",
                "collection": collection,
                "pipeline": [
                    {"$sample": {"size": min(50, strategy.get("sample_size", 20))}},
                    {"$project": {"_id": 0}}  # Exclude _id for cleaner output
                ]
            }
            
            response = await self._safe_http_call("mongodb_tool", mcp_request)
            
            if response and "result" in response:
                data = response["result"]
                if isinstance(data, list) and data:
                    logger.info(f"ðŸ“Š Fetched {len(data)} sample records from {collection}")
                    return data
                else:
                    logger.info(f"ðŸ“­ No data found in {collection}")
                    return []
            else:
                logger.warning(f"No valid response for {collection}")
                return []
                
        except Exception as e:
            logger.error(f"Error fetching data from {collection}: {e}")
            return []

    async def _ai_enhance_data_analysis(self, fetched_data: Dict, query: str, strategy: Dict) -> Dict:
        """Enhance analysis using AI with semantic understanding of the data"""
        logger.info("ðŸ§  Enhancing analysis with AI semantic understanding...")
        
        try:
            # Prepare comprehensive data summary for AI
            data_summary = self._prepare_data_summary_for_ai(fetched_data, strategy)
            
            # Create AI prompt for enhanced analysis
            ai_prompt = f"""
As an expert data analyst, provide deep insights based on this MongoDB data analysis.

Query: "{query}"
Strategy: {strategy.get('analysis_type', 'general')}

Data Summary:
{data_summary}

Provide a comprehensive analysis that includes:
1. Key patterns and trends discovered
2. User behavior insights (if applicable)
3. Business intelligence findings
4. Data quality observations
5. Actionable recommendations
6. Statistical significance of findings
7. Potential areas for deeper investigation

Focus on semantic understanding of what the data reveals about user behavior, business operations, and growth opportunities.
"""

            logger.info("ðŸ§  Consulting AI for enhanced data analysis...")
            ai_result = await app_state["ai_provider_manager"].generate_response(ai_prompt)
            
            if ai_result and ai_result.get("output"):
                ai_response = ai_result["output"]
                enhanced_analysis = {
                    "ai_insights": ai_response,
                    "data_summary": data_summary,
                    "query": query,
                    "strategy": strategy,
                    "collections_analyzed": list(fetched_data.keys()),
                    "analysis_timestamp": datetime.utcnow().isoformat()
                }
                
                logger.info("âœ¨ AI analysis complete with enhanced insights")
                return enhanced_analysis
            else:
                logger.warning("AI analysis failed, providing basic summary")
                return {
                    "data_summary": data_summary,
                    "query": query,
                    "strategy": strategy,
                    "collections_analyzed": list(fetched_data.keys()),
                    "basic_insights": "Analysis completed with available data",
                    "analysis_timestamp": datetime.utcnow().isoformat()
                }
                
        except Exception as e:
            logger.error(f"Error in AI enhanced analysis: {e}")
            return {
                "error": str(e),
                "data_summary": fetched_data,
                "query": query,
                "analysis_timestamp": datetime.utcnow().isoformat()
            }

    def _prepare_data_summary_for_ai(self, fetched_data: Dict, strategy: Dict) -> str:
        """Prepare a comprehensive summary of fetched data for AI analysis"""
        summary_parts = []
        
        for collection, data in fetched_data.items():
            if isinstance(data, list) and data:
                summary_parts.append(f"Collection '{collection}':")
                summary_parts.append(f"  - Records: {len(data)}")
                
                # Sample field analysis
                sample_record = data[0]
                summary_parts.append(f"  - Fields: {list(sample_record.keys())}")
                
                # Field type analysis
                field_types = {}
                for record in data[:5]:  # Analyze first 5 records
                    for field, value in record.items():
                        if field not in field_types:
                            field_types[field] = type(value).__name__
                
                summary_parts.append(f"  - Field types: {field_types}")
                summary_parts.append("")
        
        return "\n".join(summary_parts) if summary_parts else "No data available for analysis"

    def _intelligent_fallback_strategy(self, query: str, collections_discovery: dict) -> dict:
        """âš¡ Intelligent fallback when AI is unavailable"""
        collections = collections_discovery.get("analysis", {})
        query_lower = query.lower()
        
        # Dynamic query analysis
        query_keywords = {
            "user_focus": ["user", "customer", "people", "visitor", "member"],
            "behavior_focus": ["behavior", "activity", "engagement", "interaction", "usage"],
            "business_focus": ["sales", "revenue", "order", "purchase", "profit", "business"],
            "content_focus": ["content", "product", "item", "material", "resource"],
            "marketing_focus": ["campaign", "marketing", "promotion", "advertising"],
            "support_focus": ["support", "ticket", "help", "issue", "problem"],
            "analytics_focus": ["analyze", "analysis", "insight", "pattern", "trend", "metric"]
        }
        
        # Score collections based on relevance
        collection_scores = {}
        for collection_name, details in collections.items():
            score = 0
            count = details.get("count", 0)
            fields = details.get("sample_fields", [])
            
            # Base score from document count (logarithmic)
            if count > 0:
                score += min(50, 10 * (count ** 0.3))
            
            # Keyword matching in collection name
            for category, keywords in query_keywords.items():
                if any(keyword in query_lower for keyword in keywords):
                    if any(keyword in collection_name.lower() for keyword in keywords):
                        score += 30
            
            # Field relevance scoring
            relevant_fields = ["user_id", "timestamp", "activity", "event", "revenue", "amount"]
            score += sum(10 for field in relevant_fields if any(rf in field.lower() for rf in fields))
            
            collection_scores[collection_name] = score
        
        # Select top collections
        sorted_collections = sorted(collection_scores.items(), key=lambda x: x[1], reverse=True)
        primary_collections = [name for name, score in sorted_collections[:3] if score > 20]
        secondary_collections = [name for name, score in sorted_collections[3:6] if score > 10]
        
        # Determine strategy complexity
        if len(primary_collections) > 1 and any("user" in name.lower() for name in primary_collections):
            strategy_type = "user_behavioral_analysis"
            aggregation = "complex"
        elif any(word in query_lower for word in ["revenue", "sales", "business"]):
            strategy_type = "business_intelligence"
            aggregation = "join"
        else:
            strategy_type = "exploratory_analysis"
            aggregation = "simple"
        
        return {
            "analysis_type": strategy_type,
            "primary_collections": primary_collections,
            "secondary_collections": secondary_collections,
            "aggregation_strategy": aggregation,
            "join_relationships": self._suggest_joins(primary_collections, collections),
            "key_fields": ["_id", "user_id", "timestamp", "amount", "status", "type"],
            "insight_priorities": ["patterns", "trends", "user_behavior", "business_metrics"],
            "complexity_level": "moderate",
            "expected_output": f"Dynamic analysis of {', '.join(primary_collections)}",
            "reasoning": f"Selected based on query keywords and collection relevance scores",
            "ai_generated": False,
            "confidence": "medium"
        }

    def _suggest_joins(self, collections: list, collection_details: dict) -> list:
        """Suggest intelligent joins between collections"""
        joins = []
        
        for i, col1 in enumerate(collections):
            for col2 in collections[i+1:]:
                col1_fields = collection_details.get(col1, {}).get("sample_fields", [])
                col2_fields = collection_details.get(col2, {}).get("sample_fields", [])
                
                # Common join fields
                common_fields = set(col1_fields) & set(col2_fields)
                join_candidates = [f for f in common_fields if any(keyword in f.lower() for keyword in ["id", "user", "customer"])]
                
                if join_candidates:
                    joins.append({
                        "collection1": col1,
                        "collection2": col2, 
                        "join_field": join_candidates[0]
                    })
        
        return joins

    def _analyze_field_types(self, sample_doc: dict) -> dict:
        """Analyze field types in sample document"""
        if not sample_doc:
            return {}
            
        field_types = {}
        for field, value in sample_doc.items():
            if isinstance(value, str):
                field_types[field] = "string"
            elif isinstance(value, int):
                field_types[field] = "integer"
            elif isinstance(value, float):
                field_types[field] = "float"
            elif isinstance(value, bool):
                field_types[field] = "boolean"
            elif isinstance(value, list):
                field_types[field] = "array"
            elif isinstance(value, dict):
                field_types[field] = "object"
            else:
                field_types[field] = "unknown"
        return field_types

    def _format_collections_for_ai(self, collections_discovery: dict) -> str:
        """Format collection data for AI consumption"""
        analysis = collections_discovery.get("analysis", {})
        formatted_lines = []
        
        for collection, details in analysis.items():
            count = details.get("count", 0)
            fields = details.get("sample_fields", [])
            types = details.get("data_types", {})
            samples = details.get("sample_data", [])
            
            line = f"â€¢ {collection}: {count:,} documents"
            
            if fields:
                field_info = ", ".join([f"{field}({types.get(field, '?')})" for field in fields[:6]])
                line += f"\n  Fields: {field_info}"
            
            if samples:
                sample_preview = str(samples[0])[:100] + "..." if len(str(samples[0])) > 100 else str(samples[0])
                line += f"\n  Sample: {sample_preview}"
            
            formatted_lines.append(line)
        
        return "\n".join(formatted_lines)

    async def _ai_execute_data_fetching(self, query: str, strategy: dict) -> dict:
        """ðŸ”„ AI executes dynamic data fetching based on strategy"""
        try:
            logger.info(f"ðŸ”„ Executing {strategy.get('aggregation_strategy')} data fetching strategy...")
            
            data_context = {
                "query_context": query,
                "documents": [],
                "collections_used": [],
                "strategy_used": strategy,
                "execution_timestamp": datetime.utcnow().isoformat()
            }
            
            aggregation_strategy = strategy.get("aggregation_strategy", "simple")
            primary_collections = strategy.get("primary_collections", [])
            
            if aggregation_strategy == "complex" and len(primary_collections) > 1:
                # Complex multi-collection analysis
                data_context = await self._execute_complex_aggregation(query, strategy, data_context)
            elif aggregation_strategy == "join" and strategy.get("join_relationships"):
                # Join-based analysis
                data_context = await self._execute_join_analysis(query, strategy, data_context)
            else:
                # Simple collection-by-collection fetching
                data_context = await self._execute_simple_fetching(query, strategy, data_context)
            
            data_context["total_found"] = len(data_context["documents"])
            data_context["analysis_type"] = strategy.get("analysis_type", "dynamic_analysis")
            
            logger.info(f"âœ… Data fetching complete: {data_context['total_found']} documents from {len(data_context['collections_used'])} collections")
            return data_context
            
        except Exception as e:
            logger.error(f"Error executing data fetching: {e}")
            return {"documents": [], "collections_used": [], "total_found": 0, "error": str(e)}

    async def _execute_complex_aggregation(self, query: str, strategy: dict, data_context: dict) -> dict:
        """Execute complex multi-collection aggregation"""
        try:
            primary_collections = strategy.get("primary_collections", [])
            
            # Build dynamic aggregation pipeline based on available collections
            if len(primary_collections) >= 2:
                main_collection = primary_collections[0]
                lookup_collections = primary_collections[1:3]  # Limit to prevent complexity
                
                pipeline = []
                
                # Add lookups for related collections
                for lookup_col in lookup_collections:
                    pipeline.append({
                        "$lookup": {
                            "from": lookup_col,
                            "localField": "user_id" if "user" in lookup_col else "_id",
                            "foreignField": "user_id" if "user" in main_collection else "_id", 
                            "as": f"{lookup_col}_data"
                        }
                    })
                
                # Add intelligent field calculations
                pipeline.extend([
                    {
                        "$addFields": {
                            "total_related_records": {"$sum": [{"$size": f"${lc}_data"} for lc in lookup_collections]},
                            "analysis_timestamp": "$$NOW"
                        }
                    },
                    {"$limit": 100}
                ])
                
                async with httpx.AsyncClient() as client:
                    response = await client.post(
                        f"{config.MCP_SERVER_URL}/execute",
                        json={
                            "tool": "mongodb_aggregate",
                            "params": {"collection": main_collection, "pipeline": pipeline}
                        }
                    )
                
                if response.status_code == 200:
                    result = response.json()
                    if result.get("success") and result.get("result"):
                        documents = result["result"].get("documents", result["result"].get("result", []))
                        data_context["documents"] = documents
                        data_context["collections_used"] = [main_collection] + lookup_collections
                    data_context["aggregation_pipeline"] = pipeline
            
            return data_context
            
        except Exception as e:
            logger.error(f"Error in complex aggregation: {e}")
            return data_context

    async def _execute_join_analysis(self, query: str, strategy: dict, data_context: dict) -> dict:
        """Execute join-based analysis"""
        # Similar to complex aggregation but focused on specific joins
        return await self._execute_complex_aggregation(query, strategy, data_context)

    async def _execute_simple_fetching(self, query: str, strategy: dict, data_context: dict) -> dict:
        """Execute simple collection-by-collection fetching"""
        try:
            primary_collections = strategy.get("primary_collections", [])
            
            for collection in primary_collections[:4]:  # Reasonable limit
                try:
                    async with httpx.AsyncClient() as client:
                        response = await client.post(
                            f"{config.MCP_SERVER_URL}/execute",
                            json={
                                "tool": "mongodb_find",
                                "params": {"collection": collection, "limit": 50}
                            }
                        )
                    
                    if response.status_code == 200:
                        result = response.json()
                        if result.get("success") and result.get("result"):
                            documents = result["result"].get("documents", result["result"].get("result", []))
                            data_context["documents"].extend(documents)
                            data_context["collections_used"].append(collection)
                            
                            logger.info(f"  âœ“ Fetched {len(documents)} documents from {collection}")
                        
                except Exception as e:
                    logger.warning(f"Error fetching from {collection}: {e}")
            
            return data_context
            
        except Exception as e:
            logger.error(f"Error in simple fetching: {e}")
            return data_context

    async def _ai_enhance_data_analysis(self, query: str, data_context: dict) -> dict:
        """ðŸ§  AI enhances fetched data with intelligent analysis"""
        try:
            if not data_context.get("documents"):
                logger.warning("No documents to enhance")
                return data_context
            
            logger.info("ðŸ§  AI analyzing data for intelligent insights...")
            
            # Generate statistical summary
            data_context["statistical_summary"] = self._generate_statistical_summary(data_context["documents"])
            
            # AI-powered insight generation
            if app_state.get("ai_provider_manager"):
                try:
                    docs_summary = self._prepare_dynamic_data_summary(data_context)
                    
                    ai_prompt = f"""
Analyze this database query result and provide intelligent insights:

ORIGINAL QUERY: "{query}"
COLLECTIONS ANALYZED: {', '.join(data_context.get('collections_used', []))}
DOCUMENTS FOUND: {len(data_context['documents'])}

DATA SUMMARY:
{docs_summary}

Provide comprehensive analysis including:
1. ðŸ” Key Patterns & Trends
2. ðŸ“Š Statistical Insights  
3. ðŸ‘¥ User/Customer Behavior (if applicable)
4. ðŸ’¼ Business Intelligence
5. ðŸŽ¯ Actionable Recommendations
6. ðŸš€ Strategic Opportunities

Focus on specific data points, real numbers, and concrete insights from the actual data.
Be conversational but precise, like an expert data analyst.
"""

                    ai_result = await app_state["ai_provider_manager"].generate_response(ai_prompt)
                    
                    if ai_result and ai_result.get("output"):
                        ai_response = ai_result["output"]
                        if isinstance(ai_response, dict):
                            insights = ai_response.get("response", str(ai_response))
                        else:
                            insights = str(ai_response)
                        
                        data_context["ai_insights"] = insights
                        data_context["intelligence_level"] = "ai_enhanced"
                        logger.info("âœ¨ AI insights generated successfully")
                        
                except Exception as e:
                    logger.warning(f"AI enhancement failed: {e}")
                    data_context["ai_enhancement_error"] = str(e)
            
            return data_context
            
        except Exception as e:
            logger.error(f"Error enhancing data analysis: {e}")
            data_context["enhancement_error"] = str(e)
            return data_context

    def _prepare_dynamic_data_summary(self, data_context: dict) -> str:
        """Prepare dynamic data summary for AI analysis"""
        try:
            documents = data_context.get("documents", [])
            collections_used = data_context.get("collections_used", [])
            
            if not documents:
                return "No documents found matching the query criteria."
            
            summary_parts = [
                f"ðŸ“Š Analysis Summary:",
                f"Collections: {', '.join(collections_used)}",
                f"Documents: {len(documents):,}",
                f"Strategy: {data_context.get('strategy_used', {}).get('analysis_type', 'unknown')}",
                ""
            ]
            
            # Sample key data points
            for i, doc in enumerate(documents[:3]):
                summary_parts.append(f"Sample {i+1}:")
                for key, value in doc.items():
                    if key != '_id' and not key.startswith('_'):
                        value_str = str(value)[:50] + "..." if len(str(value)) > 50 else str(value)
                        summary_parts.append(f"  {key}: {value_str}")
                summary_parts.append("")
                
            # Add statistical overview
            stats = data_context.get("statistical_summary", {})
            if stats:
                summary_parts.extend([
                    "ðŸ“ˆ Statistical Overview:",
                    f"Total fields analyzed: {len(stats.get('field_analysis', {}))}",
                    f"Data completeness: {stats.get('avg_completeness', 0):.1f}%"
                ])
            
            return "\n".join(summary_parts)
            
        except Exception as e:
            return f"Error preparing summary: {e}"

    def _generate_statistical_summary(self, documents: list) -> dict:
        """Generate statistical summary of documents"""
        if not documents:
            return {}
        
        try:
            summary = {
                "total_documents": len(documents),
                "field_analysis": {},
                "data_types": {},
                "completeness": {},
                "generation_timestamp": datetime.utcnow().isoformat()
            }
            
            # Analyze all unique fields
            all_fields = set()
            for doc in documents:
                all_fields.update(doc.keys())
            
            for field in all_fields:
                values = [doc.get(field) for doc in documents if field in doc]
                non_null_values = [v for v in values if v is not None and v != ""]
                
                summary["field_analysis"][field] = {
                    "present_in": len(non_null_values),
                    "coverage_percent": (len(non_null_values) / len(documents)) * 100,
                    "unique_values": len(set(str(v) for v in non_null_values)) if non_null_values else 0
                }
                
                if non_null_values:
                    sample_value = non_null_values[0]
                    summary["data_types"][field] = type(sample_value).__name__
            
            # Overall completeness
            total_coverage = sum(fa["coverage_percent"] for fa in summary["field_analysis"].values())
            summary["avg_completeness"] = total_coverage / len(summary["field_analysis"]) if summary["field_analysis"] else 0
            
            return summary
            
        except Exception as e:
            logger.error(f"Error generating statistical summary: {e}")
            return {"error": str(e)}



    async def _create_intelligent_natural_response(self, query: str, data_context: dict, result: dict) -> str:
        """Create intelligent natural language response combining data analysis with AI"""
        try:
            # Use the enhanced data analysis to create intelligent response
            enhanced_response = self._create_intelligent_response_from_data(query, data_context)
            
            # If we have AI provider manager, enhance the response further
            if app_state.get("ai_provider_manager"):
                try:
                    ai_prompt = f"""
Enhance this data analysis response to be more conversational and insightful:

Original Query: "{query}"
Current Response: "{enhanced_response}"

Make it more engaging, conversational, and add strategic insights while keeping all the specific data points.
"""
                    ai_result = await app_state["ai_provider_manager"].generate_response(ai_prompt)
                    if ai_result and ai_result.get("output"):
                        ai_response = ai_result["output"]
                        if isinstance(ai_response, dict):
                            return ai_response.get("response", enhanced_response)
                        return str(ai_response)
                except Exception as e:
                    logger.warning(f"AI enhancement failed, using original response: {e}")
            
            return enhanced_response
            
        except Exception as e:
            logger.error(f"Error creating intelligent natural response: {e}")
            return f"I analyzed your query '{query}' but encountered an issue generating the response. Please try rephrasing your question."

    async def _cache_data_analysis(self, query: str, data: dict) -> None:
        """Cache data analysis with intelligent mapping for future use"""
        try:
            # Create semantic cache key
            cache_key = f"data_analysis:{hash(query) % 10000000}"
            
            # Store the analysis with metadata
            cache_entry = {
                "original_query": query,
                "data": data,
                "timestamp": datetime.now().isoformat(),
                "collections": data.get("collections_used", []),
                "analysis_type": data.get("analysis_type", "general"),
                "document_count": data.get("total_found", 0)
            }
            
            # Cache with extended TTL for data analysis (2 hours)
            if app_state.get("ai_cache"):
                await app_state["ai_cache"].set(cache_key, cache_entry, ttl=7200)
                
                # Store cache key mapping for semantic search
                await self._store_cache_key_mapping(query, cache_key)
                
                logger.info(f"ðŸ’¾ Cached data analysis: {cache_key} ({data.get('total_found', 0)} documents)")
            else:
                logger.warning("Cache not available, skipping cache storage")
            
        except Exception as e:
            logger.error(f"Error caching data analysis: {e}")

    async def _store_cache_key_mapping(self, query: str, cache_key: str) -> None:
        """Store mapping of queries to cache keys for semantic search"""
        try:
            if not app_state.get("ai_cache"):
                return
                
            mapping_key = "cache_query_mappings"
            
            # Get existing mappings
            existing_mappings = await app_state["ai_cache"].get(mapping_key) or []
            
            # Add new mapping
            new_mapping = {
                "query": query,
                "cache_key": cache_key,
                "timestamp": datetime.now().isoformat(),
                "query_words": query.lower().split()
            }
            
            existing_mappings.append(new_mapping)
            
            # Keep only last 50 mappings to prevent memory issues
            if len(existing_mappings) > 50:
                existing_mappings = existing_mappings[-50:]
            
            # Store updated mappings (cache for 24 hours)
            await app_state["ai_cache"].set(mapping_key, existing_mappings, ttl=86400)
            
        except Exception as e:
            logger.error(f"Error storing cache key mapping: {e}")

    async def _get_cache_analysis_keys(self) -> list:
        """Get all available cache analysis keys"""
        try:
            if not app_state.get("ai_cache"):
                return []
                
            mapping_key = "cache_query_mappings"
            mappings = await app_state["ai_cache"].get(mapping_key) or []
            return mappings
            
        except Exception as e:
            logger.error(f"Error getting cache analysis keys: {e}")
            return []

    async def _find_semantically_similar_cache(self, query: str, cache_mappings: list) -> dict:
        """Use AI to find semantically similar cached analyses"""
        try:
            if not cache_mappings:
                return None
            
            query_lower = query.lower()
            query_words = set(query_lower.split())
            
            # First try simple word matching for speed
            best_match = None
            best_score = 0.0
            
            for mapping in cache_mappings:
                cached_query = mapping["query"].lower()
                cached_words = set(mapping.get("query_words", cached_query.split()))
                
                # Calculate word overlap score
                overlap = len(query_words.intersection(cached_words))
                total_words = len(query_words.union(cached_words))
                word_score = overlap / total_words if total_words > 0 else 0.0
                
                # Boost score for key terms
                key_terms = ["user", "customer", "campaign", "marketing", "content", "data", "analysis"]
                key_overlap = sum(1 for term in key_terms if term in query_lower and term in cached_query)
                
                final_score = word_score + (key_overlap * 0.2)
                
                if final_score > best_score and final_score > 0.3:  # Minimum threshold
                    best_score = final_score
                    best_match = {
                        "cache_key": mapping["cache_key"],
                        "original_query": mapping["query"],
                        "similarity": final_score,
                        "timestamp": mapping["timestamp"]
                    }
            
            if best_match:
                logger.info(f"ðŸŽ¯ Found similar cache: {best_match['similarity']:.2f} similarity")
                return best_match
            
            return None
            
        except Exception as e:
            logger.error(f"Error finding semantically similar cache: {e}")
            return None

    async def _call_ai_provider(self, prompt: str) -> str:
        """Call AI provider for intelligent analysis"""
        try:
            if self.ai_provider_manager:
                response = await self.ai_provider_manager.generate_response(prompt)
                return response
            else:
                # Fallback to basic analysis
                return '{"fully_covers": false, "partially_covers": false, "missing_aspects": [], "confidence": 0.5, "reasoning": "AI provider not available"}'
                
        except Exception as e:
            logger.error(f"Error calling AI provider: {e}")
            return '{"fully_covers": false, "partially_covers": false, "missing_aspects": [], "confidence": 0.0, "reasoning": "Error in AI analysis"}'

    async def _generate_ai_powered_response(self, query: str, real_data: dict) -> str:
        """Use AI to generate intelligent, conversational responses like ChatGPT/Claude"""
        try:
            # Prepare real data summary for AI
            data_summary = self._prepare_data_summary(real_data)
            
            # Create intelligent prompt for AI
            ai_prompt = f"""
You are an intelligent business data analyst. A user asked: "{query}"

Here's the ACTUAL data from their system:
{data_summary}

Provide an intelligent, conversational response that:
1. Uses the REAL data (actual numbers, IDs, content IDs, etc.)
2. Gives specific insights and actionable recommendations
3. Is conversational and helpful like ChatGPT/Claude
4. Includes HOW-TO implementations when relevant
5. Shows actual examples from their data

Be specific, intelligent, and helpful. Reference actual data points, not generic statistics.
"""

            # Use AI provider manager for intelligent response
            if app_state.get("ai_provider_manager"):
                ai_result = await app_state["ai_provider_manager"].generate_response(
                    ai_prompt,
                    {"query": query, "data": real_data}
                )
                
                if ai_result and ai_result.get("output"):
                    response_text = ai_result["output"]
                    if isinstance(response_text, dict):
                        return response_text.get("response", str(response_text))
                    return str(response_text)
            
            # Fallback: Create intelligent response based on actual data
            return self._create_intelligent_response_from_data(query, real_data)
            
        except Exception as e:
            logger.error(f"AI response generation failed: {e}")
            return self._create_intelligent_response_from_data(query, real_data)

    def _prepare_data_summary(self, real_data: dict) -> str:
        """Prepare real data summary for AI analysis"""
        try:
            documents = real_data.get("documents", [])
            analysis_type = real_data.get("analysis_type", "general")
            
            if not documents:
                return "No specific data found in the collections."
            
            if analysis_type == "customer_analysis":
                # Customer activity data
                summary_parts = ["Customer Activity Analysis:"]
                for i, doc in enumerate(documents[:5]):
                    user_id = doc.get("_id", f"user_{i+1}")
                    events = doc.get("events", 0)
                    latest = doc.get("latest_activity", "unknown")
                    summary_parts.append(f"- User {user_id}: {events} activities, last seen {latest}")
                return "\n".join(summary_parts)
                
            elif analysis_type == "content_analysis":
                # Content popularity data
                summary_parts = ["Content Popularity Analysis:"]
                for i, doc in enumerate(documents[:5]):
                    content_id = doc.get("_id", f"content_{i+1}")
                    views = doc.get("views", 0)
                    summary_parts.append(f"- Content {content_id}: {views} views")
                return "\n".join(summary_parts)
                
            elif analysis_type == "marketing_analysis":
                # Marketing campaign data
                summary_parts = ["Marketing Campaigns:"]
                for i, doc in enumerate(documents[:3]):
                    campaign_id = doc.get("_id", f"campaign_{i+1}")
                    name = doc.get("name", "Unknown Campaign")
                    status = doc.get("status", "unknown")
                    summary_parts.append(f"- {name} (ID: {campaign_id}): Status {status}")
                return "\n".join(summary_parts)
                
            else:
                # General overview
                summary_parts = ["Database Overview:"]
                for doc in documents:
                    if "collection" in doc:
                        collection = doc["collection"]
                        count = doc["count"]
                        summary_parts.append(f"- {collection}: {count:,} records")
                return "\n".join(summary_parts)
                
        except Exception as e:
            logger.error(f"Error preparing data summary: {e}")
            return f"Data summary error: {e}"

    def _create_intelligent_response_from_data(self, query: str, real_data: dict) -> str:
        """Create DEEP INTELLIGENT response using comprehensive user analysis (not just counts)"""
        try:
            documents = real_data.get("documents", [])
            analysis_type = real_data.get("analysis_type", "general")
            collections_used = real_data.get("collections_used", [])
            detailed_analysis = real_data.get("detailed_user_analysis", {})
            
            if not documents:
                return f"I analyzed your database collections {collections_used} but didn't find specific data matching your query '{query}'. The collections exist but may need different query parameters."
            
            query_lower = query.lower()
            
            # DEEP USER BEHAVIORAL ANALYSIS with comprehensive insights
            if analysis_type == "deep_user_behavioral_analysis" and documents:
                
                # Extract rich insights from the deep analysis
                total_users = len(documents)
                high_engagement = detailed_analysis.get("high_engagement_users", [])
                conversion_champions = detailed_analysis.get("conversion_champions", [])
                at_risk_users = detailed_analysis.get("at_risk_users", [])
                new_users = detailed_analysis.get("new_users", [])
                loyal_customers = detailed_analysis.get("loyal_customers", [])
                multi_device_users = detailed_analysis.get("multi_device_users", [])
                
                # Calculate comprehensive metrics
                avg_events = sum(user.get("engagement_metrics", {}).get("total_events", 0) for user in documents) / total_users
                avg_lifetime = sum(user.get("customer_journey", {}).get("customer_lifetime_days", 0) for user in documents) / total_users
                total_revenue = sum(user.get("business_value", {}).get("revenue_generated", 0) for user in documents)
                
                # Revenue per user insights
                revenue_users = [u for u in documents if u.get("business_value", {}).get("revenue_generated", 0) > 0]
                avg_revenue_per_paying_user = total_revenue / len(revenue_users) if revenue_users else 0
                
                # Device and location diversity
                all_devices = set()
                all_locations = set()
                for user in documents:
                    all_devices.update(user.get("technology_profile", {}).get("devices_used", []))
                    all_locations.update(user.get("geographic_profile", {}).get("locations", []))
                
                response = f"ðŸ“Š **Deep User Intelligence Analysis** (analyzed {total_users} users with complete behavioral profiles)\\n\\n"
                
                # ENGAGEMENT INTELLIGENCE
                response += f"**ðŸŽ¯ Engagement Intelligence:**\\n"
                response += f"â€¢ **High Performers**: {len(high_engagement)} users ({len(high_engagement)/total_users*100:.1f}%) showing exceptional engagement\\n"
                response += f"â€¢ **Average Activity**: {avg_events:.1f} events per user over {avg_lifetime:.1f} days\\n"
                response += f"â€¢ **Conversion Excellence**: {len(conversion_champions)} users ({len(conversion_champions)/total_users*100:.1f}%) converting at >10% rate\\n"
                response += f"â€¢ **Risk Assessment**: {len(at_risk_users)} users identified as at-risk (low recent activity)\\n\\n"
                
                # TOP PERFORMER DEEP DIVE
                if high_engagement:
                    top_user = high_engagement[0]
                    user_id = top_user.get("user_id", "Unknown")
                    events = top_user.get("engagement_metrics", {}).get("total_events", 0)
                    days_active = top_user.get("engagement_metrics", {}).get("days_active", 0)
                    revenue = top_user.get("business_value", {}).get("revenue_generated", 0)
                    devices = top_user.get("technology_profile", {}).get("devices_used", [])
                    locations = top_user.get("geographic_profile", {}).get("locations", [])
                    
                    response += f"**ðŸŒŸ Top User Deep Profile (User: {user_id}):**\\n"
                    response += f"â€¢ **Activity**: {events} total events across {days_active} active days\\n"
                    response += f"â€¢ **Value**: ${revenue:.2f} revenue generated\\n"
                    response += f"â€¢ **Technology**: Uses {len(devices)} device types: {', '.join(devices)}\\n"
                    response += f"â€¢ **Geographic**: Active in {len(locations)} locations: {', '.join(locations)}\\n\\n"
                
                # CUSTOMER LIFETIME INTELLIGENCE
                response += f"**ðŸ“ˆ Customer Lifecycle Intelligence:**\\n"
                response += f"â€¢ **New Users**: {len(new_users)} users joined recently (potential for onboarding optimization)\\n"
                response += f"â€¢ **Loyal Base**: {len(loyal_customers)} repeat customers with 60+ day relationships\\n"
                response += f"â€¢ **Multi-Channel**: {len(multi_device_users)} users engaging across multiple devices\\n"
                response += f"â€¢ **Revenue Performance**: ${avg_revenue_per_paying_user:.2f} average per paying user\\n\\n"
                
                # TECHNOLOGY & GEOGRAPHIC INSIGHTS
                response += f"**ðŸŒ Technology & Geographic Intelligence:**\\n"
                response += f"â€¢ **Device Ecosystem**: {len(all_devices)} unique device types: {', '.join(list(all_devices)[:5])}{'...' if len(all_devices) > 5 else ''}\\n"
                response += f"â€¢ **Geographic Reach**: {len(all_locations)} locations: {', '.join(list(all_locations)[:5])}{'...' if len(all_locations) > 5 else ''}\\n\\n"
                
                # DETAILED USER EXAMPLES
                if "specific" in query_lower or "detail" in query_lower or "example" in query_lower:
                    response += f"**ðŸ‘¥ Detailed User Examples:**\\n"
                    for i, user in enumerate(documents[:3]):
                        user_id = user.get("user_id", f"User_{i+1}")
                        engagement = user.get("engagement_metrics", {})
                        journey = user.get("customer_journey", {})
                        business = user.get("business_value", {})
                        
                        response += f"\\n**User {user_id}:**\\n"
                        response += f"  â€¢ Events: {engagement.get('total_events', 0)} | Days Active: {engagement.get('days_active', 0)}\\n"
                        response += f"  â€¢ Engagement: {engagement.get('engagement_level', 'unknown')} | Bounce Rate: {engagement.get('bounce_rate', 0):.1%}\\n"
                        response += f"  â€¢ Customer Since: {journey.get('customer_lifetime_days', 0):.0f} days | Revenue: ${business.get('revenue_generated', 0):.2f}\\n"
                        response += f"  â€¢ Purchases: {user.get('behavioral_analysis', {}).get('purchases', 0)} | Conversion: {user.get('behavioral_analysis', {}).get('purchase_conversion', 0):.1%}\\n"
                
                # STRATEGIC RECOMMENDATIONS
                response += f"\\n\\n**ðŸš€ Strategic Intelligence & Recommendations:**\\n"
                
                if len(high_engagement) > 0:
                    response += f"1. **Champion Strategy**: Your {len(high_engagement)} high-engagement users are your growth engine. Analyze their behavioral patterns to identify what drives their exceptional engagement.\\n"
                
                if len(at_risk_users) > 0:
                    response += f"2. **Risk Mitigation**: {len(at_risk_users)} users show declining engagement. Implement re-engagement campaigns based on their historical preferences.\\n"
                
                if len(multi_device_users) > 0:
                    response += f"3. **Cross-Platform Optimization**: {len(multi_device_users)} users engage across multiple devices. Ensure seamless experiences across all platforms.\\n"
                
                if avg_revenue_per_paying_user > 0:
                    response += f"4. **Revenue Expansion**: Average paying user generates ${avg_revenue_per_paying_user:.2f}. Focus on converting non-paying engaged users to increase overall revenue.\\n"
                
                response += f"\\n**ðŸ’¡ Implementation Focus:**\\n"
                response += f"â€¢ Personalize experiences based on individual user technology and location preferences\\n"
                response += f"â€¢ Create targeted campaigns for each engagement segment\\n"
                response += f"â€¢ Implement predictive analytics to identify future high-value users\\n"
                response += f"â€¢ Develop retention strategies specific to user behavioral patterns"
                
                return response
                
                return response
                
            elif analysis_type == "marketing_analysis" and documents:
                # Real marketing analysis
                response = f"Found {len(documents)} marketing campaigns in your system. "
                
                campaign_details = []
                for campaign in documents[:3]:
                    name = campaign.get("name", "Unnamed Campaign")
                    status = campaign.get("status", "unknown")
                    campaign_id = campaign.get("_id", "N/A")
                    campaign_details.append(f"- {name} (ID: {campaign_id}): {status}")
                
                if campaign_details:
                    response += "\\nActive campaigns:\\n" + "\\n".join(campaign_details)
                
                response += "\\n\\n**Marketing Optimization:**\\n"
                response += "1. Track performance metrics for each campaign\\n"
                response += "2. A/B test different campaign approaches\\n"
                response += "3. Segment audiences based on campaign response"
                
                return response
                
            else:
                # General database overview with real counts
                response = "Here's what I found in your database:\\n\\n"
                
                for doc in documents:
                    if "collection" in doc:
                        collection = doc["collection"]
                        count = doc["count"]
                        response += f"ðŸ“Š {collection}: {count:,} records\\n"
                
                response += "\\n**Next Steps:**\\n"
                response += "1. Specify which data you'd like to analyze in detail\\n"
                response += "2. Ask about specific metrics or patterns\\n"
                response += "3. Request insights on particular collections"
                
                return response
                
        except Exception as e:
            logger.error(f"Error creating intelligent response: {e}")
            return f"I found data in your system but encountered an error analyzing it: {e}. Please try a more specific query."

    async def _generate_contextual_fallback(self, query: str, result: dict) -> str:
        """Generate contextual fallback response when AI fails"""
        try:
            query_lower = query.lower()
            
            if any(word in query_lower for word in ["customer", "user"]):
                return f"I'd be happy to analyze your customer data! I can see your user_activity collection with real engagement data. Could you try asking something like 'show me my top customers' or 'analyze user engagement patterns'?"
                
            elif any(word in query_lower for word in ["content", "popular", "product"]):
                return f"I can analyze your content performance! Your system has activity data that shows what content is most popular. Try asking 'what content is most popular' or 'show me top performing content'."
                
            elif any(word in query_lower for word in ["campaign", "marketing"]):
                return f"I found {200} marketing campaigns in your system! Ask me things like 'show me my marketing campaigns' or 'which campaigns are performing best'."
                
            else:
                return f"I can analyze your real business data! You have user activity (20,000+ records), marketing campaigns (200), and content data. Try asking specific questions like 'who are my top customers' or 'what content is most popular'."
                
        except Exception as e:
            return f"I'm ready to analyze your data - just ask me something specific about customers, content, or campaigns!"

    async def _check_conversational_cache(self, query: str) -> dict:
        """Check if query can use existing conversational cache"""
        try:
            logger.info(f"ðŸ” Checking conversational cache for: '{query}'")
            
            # Get query entities and intent
            entities = self._extract_query_entities(query)
            intent = self._get_query_intent(query)
            
            # Look for related cached analyses
            cache_keys = await self.state_manager.redis_client.keys("conv_cache:*")
            
            best_match = {
                "found": False,
                "cache_key": None,
                "cached_data": None,
                "overlap_score": 0,
                "needs_additional_data": False,
                "missing_entities": [],
                "can_adapt": False
            }
            
            for cache_key in cache_keys:
                try:
                    cached_data = await self.state_manager.redis_client.get(cache_key)
                    if not cached_data:
                        continue
                        
                    cache_entry = json.loads(cached_data)
                    cached_entities = cache_entry.get("entities", [])
                    cached_intent = cache_entry.get("intent", "")
                    
                    # Calculate entity overlap
                    entity_overlap = len(set(entities).intersection(set(cached_entities)))
                    total_entities = len(set(entities).union(set(cached_entities)))
                    overlap_score = entity_overlap / total_entities if total_entities > 0 else 0
                    
                    # Check intent similarity
                    intent_match = intent == cached_intent or self._are_intents_compatible(intent, cached_intent)
                    
                    if overlap_score > 0.3 and intent_match:  # 30% entity overlap + compatible intent
                        if overlap_score > best_match["overlap_score"]:
                            missing_entities = list(set(entities) - set(cached_entities))
                            
                            best_match = {
                                "found": True,
                                "cache_key": cache_key,
                                "cached_data": cache_entry,
                                "overlap_score": overlap_score,
                                "needs_additional_data": len(missing_entities) > 0,
                                "missing_entities": missing_entities,
                                "can_adapt": overlap_score > 0.7 or len(missing_entities) == 0
                            }
                            
                except json.JSONDecodeError:
                    continue
                except Exception as e:
                    logger.error(f"Error processing cache entry {cache_key}: {e}")
                    continue
            
            if best_match["found"]:
                logger.info(f"âœ¨ Found conversational cache match: {best_match['overlap_score']:.2f} overlap, missing: {best_match['missing_entities']}")
            else:
                logger.info(f"âŒ No suitable conversational cache found")
                
            return best_match
            
        except Exception as e:
            logger.error(f"Error checking conversational cache: {e}")
            return {"found": False, "needs_additional_data": False, "missing_entities": []}

    async def _fetch_and_combine_analysis(self, query: str, cached_analysis: dict) -> dict:
        """Fetch missing data and combine with cached analysis"""
        try:
            logger.info(f"ðŸ”„ Fetching additional data for: {cached_analysis['missing_entities']}")
            
            # Create targeted query for missing entities
            missing_entities = cached_analysis["missing_entities"]
            targeted_query = self._create_targeted_query(missing_entities)
            
            # Execute minimal workflow for missing data only
            request_id = str(uuid.uuid4())
            understanding = await self._understand_query(targeted_query)
            approach = await self._decide_approach(targeted_query, understanding)
            new_result = await self._execute_with_ai(request_id, targeted_query, approach, understanding)
            
            # Extract new data context
            new_data_context = await self._fetch_real_data_for_query(query)
            
            # Combine cached analysis with new data
            cached_data = cached_analysis["cached_data"]
            combined_context = self._merge_data_contexts(cached_data["data_context"], new_data_context)
            
            # Generate new response using combined context
            combined_response = await self._create_intelligent_natural_response(query, combined_context, new_result)
            
            # Update cache with combined analysis
            await self._update_conversational_cache(cached_analysis["cache_key"], query, combined_context, combined_response)
            
            return {
                "request_id": request_id,
                "status": "completed",
                "understanding": understanding,
                "approach": approach,
                "human_response": combined_response,
                "result": new_result,
                "from_cache": False,
                "cache_enhanced": True,
                "timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error fetching and combining analysis: {e}")
            # Fallback to full analysis
            return await self.process_query(query)

    async def _adapt_cached_response(self, query: str, cached_analysis: dict) -> dict:
        """Generate new response from cached analysis without re-fetching"""
        try:
            logger.info(f"âœ¨ Adapting cached response for: '{query}'")
            
            cached_data = cached_analysis["cached_data"]
            cached_context = cached_data["data_context"]
            
            # Generate new response with different phrasing/perspective
            adapted_response = await self._create_intelligent_natural_response(query, cached_context, cached_data["result"])
            
            # Create response structure
            return {
                "request_id": str(uuid.uuid4()),
                "status": "completed",
                "understanding": cached_data["understanding"],
                "approach": cached_data["approach"],
                "human_response": adapted_response,
                "result": cached_data["result"],
                "from_cache": True,
                "cache_adapted": True,
                "timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error adapting cached response: {e}")
            # Fallback to cached response
            return cached_analysis["cached_data"]

    async def _save_conversational_cache(self, query: str, result: dict, human_response: str, data_context: dict):
        """Save analysis context to conversational cache"""
        try:
            entities = self._extract_query_entities(query)
            intent = self._get_query_intent(query)
            
            cache_entry = {
                "query": query,
                "entities": entities,
                "intent": intent,
                "data_context": data_context,
                "result": result,
                "human_response": human_response,
                "understanding": {},  # Will be filled from process_query
                "approach": {},      # Will be filled from process_query
                "timestamp": datetime.utcnow().isoformat(),
                "usage_count": 1
            }
            
            # Create cache key based on entities
            cache_key = f"conv_cache:{':'.join(sorted(entities))}"
            
            # Save with extended TTL for conversational context
            await self.state_manager.redis_client.setex(
                cache_key,
                config.RESULT_CACHE_TTL * 2,  # Double TTL for conversational cache
                json.dumps(cache_entry)
            )
            
            logger.info(f"ðŸ’¾ Saved conversational cache: {cache_key}")
            
        except Exception as e:
            logger.error(f"Error saving conversational cache: {e}")

    async def _update_conversational_cache(self, cache_key: str, query: str, combined_context: dict, response: str):
        """Update existing cache with combined analysis"""
        try:
            cached_data = await self.state_manager.redis_client.get(cache_key)
            if cached_data:
                cache_entry = json.loads(cached_data)
                
                # Update with new entities and combined context
                new_entities = self._extract_query_entities(query)
                cache_entry["entities"] = list(set(cache_entry["entities"] + new_entities))
                cache_entry["data_context"] = combined_context
                cache_entry["human_response"] = response
                cache_entry["usage_count"] = cache_entry.get("usage_count", 0) + 1
                cache_entry["last_updated"] = datetime.utcnow().isoformat()
                
                await self.state_manager.redis_client.setex(
                    cache_key,
                    config.RESULT_CACHE_TTL * 2,
                    json.dumps(cache_entry)
                )
                
                logger.info(f"ðŸ”„ Updated conversational cache: {cache_key}")
                
        except Exception as e:
            logger.error(f"Error updating conversational cache: {e}")

    def _extract_query_entities(self, query: str) -> list:
        """Extract key entities from query for cache matching"""
        query_lower = query.lower()
        entities = []
        
        # Business entities
        if any(word in query_lower for word in ["customer", "user", "client"]):
            entities.append("customers")
        if any(word in query_lower for word in ["product", "item", "catalog", "inventory"]):
            entities.append("products")
        if any(word in query_lower for word in ["order", "sale", "purchase", "transaction"]):
            entities.append("orders")
        if any(word in query_lower for word in ["campaign", "marketing", "advertising"]):
            entities.append("campaigns")
        if any(word in query_lower for word in ["employee", "staff", "team"]):
            entities.append("employees")
        if any(word in query_lower for word in ["shipment", "delivery", "shipping"]):
            entities.append("shipments")
        if any(word in query_lower for word in ["warehouse", "facility", "location"]):
            entities.append("warehouses")
        if any(word in query_lower for word in ["ticket", "support", "help"]):
            entities.append("support")
        if any(word in query_lower for word in ["activity", "engagement", "behavior"]):
            entities.append("activity")
        if any(word in query_lower for word in ["content", "digital", "media"]):
            entities.append("content")
        
        return entities

    def _get_query_intent(self, query: str) -> str:
        """Determine query intent for cache matching"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ["how many", "count", "number of", "total"]):
            return "count"
        elif any(word in query_lower for word in ["show me", "list", "display", "view"]):
            return "list"
        elif any(word in query_lower for word in ["compare", "vs", "versus", "difference"]):
            return "compare"
        elif any(word in query_lower for word in ["analyze", "analysis", "insights", "patterns"]):
            return "analyze"
        elif any(word in query_lower for word in ["performance", "metrics", "kpi", "dashboard"]):
            return "performance"
        elif any(word in query_lower for word in ["largest", "biggest", "most", "highest", "top"]):
            return "ranking"
        elif any(word in query_lower for word in ["overview", "summary", "report"]):
            return "overview"
        elif any(word in query_lower for word in ["trend", "growth", "change", "over time"]):
            return "trends"
        else:
            return "general"

    def _are_intents_compatible(self, intent1: str, intent2: str) -> bool:
        """Check if two intents are compatible for cache reuse"""
        compatible_groups = [
            {"count", "list", "overview"},
            {"analyze", "performance", "trends"},
            {"compare", "ranking"},
        ]
        
        for group in compatible_groups:
            if intent1 in group and intent2 in group:
                return True
        
        return intent1 == intent2

    def _create_targeted_query(self, missing_entities: list) -> str:
        """Create minimal query to fetch only missing entities"""
        if len(missing_entities) == 1:
            entity = missing_entities[0]
            return f"How many {entity} do I have?"
        else:
            entities_str = ", ".join(missing_entities)
            return f"Show me data for {entities_str}"

    def _merge_data_contexts(self, cached_context: dict, new_context: dict) -> dict:
        """Merge cached data context with new data context"""
        merged = cached_context.copy()
        
        # Merge collections
        merged["collections"].update(new_context.get("collections", {}))
        
        # Merge business metrics
        merged["business_metrics"].update(new_context.get("business_metrics", {}))
        
        # Update totals
        merged["total_records"] = sum(merged["collections"].values())
        
        # Merge analysis results
        if "analysis_results" in new_context:
            if "analysis_results" not in merged:
                merged["analysis_results"] = {}
            merged["analysis_results"].update(new_context["analysis_results"])
        
        return merged

    def _generate_contextual_response(self, query: str, data_context: dict) -> str:
        """Generate contextual response based on data without hardcoded templates"""
        try:
            collections = data_context.get("collections", {})
            metrics = data_context.get("business_metrics", {})
            total_records = data_context.get("total_records", 0)
            
            # Analyze query intent dynamically
            query_lower = query.lower()
            
            # Build response based on actual data and query context
            if not collections and not metrics:
                return f"I can see you're interested in {self._extract_query_subject(query)}. While I'm connecting to your data sources, I'd be happy to help you think through the strategy and approach for getting the insights you need."
            
            # Generate SPECIFIC responses based on what they're actually asking
            if "how many" in query_lower and "customer" in query_lower:
                if metrics.get("users"):
                    return f"You have {metrics['users']:,} customers in your database."
                else:
                    return "Let me check your customer data to get you the exact count."
            
            elif "how many" in query_lower and ("order" in query_lower or "sale" in query_lower):
                if metrics.get("orders"):
                    return f"Your business has processed {metrics['orders']:,} orders."
                else:
                    return "Let me look up your order data to get you the exact number."
            
            elif "how many" in query_lower and "product" in query_lower:
                if metrics.get("products"):
                    return f"You have {metrics['products']:,} products in your catalog."
                else:
                    return "Let me check your product database for the exact count."
            
            elif "show me" in query_lower or "list" in query_lower:
                data_points = []
                if metrics.get("users"):
                    data_points.append(f"{metrics['users']:,} customers")
                if metrics.get("orders"):
                    data_points.append(f"{metrics['orders']:,} orders")
                if metrics.get("products"):
                    data_points.append(f"{metrics['products']:,} products")
                if metrics.get("campaigns"):
                    data_points.append(f"{metrics['campaigns']} marketing campaigns")
                
                if data_points:
                    return f"Here's what I found in your data: {', '.join(data_points)}. What specific aspect would you like me to explore further?"
                else:
                    return f"I can see {total_records:,} records across {len(collections)} collections. What specific information are you looking for?"
            
            elif "largest" in query_lower or "biggest" in query_lower:
                if "product" in query_lower and metrics.get("products"):
                    return f"Your product catalog contains {metrics['products']:,} items. To find the largest category, I'd need to analyze the product data by category groupings."
                elif "customer" in query_lower and metrics.get("users"):
                    return f"Based on your {metrics['users']:,} customers, I can help analyze which customer segments are largest by various criteria."
                else:
                    return "I can help you identify the largest categories in your data. What specific area are you most interested in?"
            
            elif "performance" in query_lower or "analytics" in query_lower:
                performance_items = []
                if metrics.get("users") and metrics.get("orders"):
                    ratio = metrics['orders'] / metrics['users']
                    performance_items.append(f"Customer engagement: {ratio:.1f} orders per customer")
                if metrics.get("products"):
                    performance_items.append(f"Product catalog: {metrics['products']:,} items")
                if metrics.get("campaigns"):
                    performance_items.append(f"Marketing activity: {metrics['campaigns']} campaigns")
                
                if performance_items:
                    return f"Here are your key performance indicators: {'; '.join(performance_items)}."
                else:
                    return f"Your system tracks {total_records:,} data points across multiple areas. What specific performance metrics would you like to see?"
            
            elif "business" in query_lower and ("overview" in query_lower or "summary" in query_lower):
                data_points = []
                if metrics.get("users"):
                    data_points.append(f"{metrics['users']:,} customers")
                if metrics.get("orders"):
                    data_points.append(f"{metrics['orders']:,} orders")
                if metrics.get("products"):
                    data_points.append(f"{metrics['products']:,} products")
                if metrics.get("campaigns"):
                    data_points.append(f"{metrics['campaigns']} campaigns")
                
                if data_points:
                    return f"Your business overview: {', '.join(data_points)}. This gives you a solid foundation for growth and optimization."
                else:
                    return f"Your business operates with {total_records:,} data points across {len(collections)} different areas."
            
            # Default fallback for unmatched queries
            else:
                data_points = []
                if metrics.get("users"):
                    data_points.append(f"{metrics['users']:,} users")
                if metrics.get("orders"):
                    data_points.append(f"{metrics['orders']:,} orders")
                if metrics.get("products"):
                    data_points.append(f"{metrics['products']:,} products")
                if metrics.get("campaigns"):
                    data_points.append(f"{metrics['campaigns']} campaigns")
                
                if data_points:
                    data_summary = ", ".join(data_points)
                    return f"Looking at your {data_summary}, I can help you with specific analysis. What would you like to know more about?"
                else:
                    return f"I can see you have {total_records:,} records across {len(collections)} data sources. What specific information are you looking for?"
                
        except Exception as e:
            logger.error(f"Error generating contextual response: {e}")
            return "That's an interesting question. Let me analyze your data to provide you with specific insights and recommendations."

    def _extract_query_subject(self, query: str) -> str:
        """Extract the main subject/topic from the query"""
        query_lower = query.lower()
        
        if "customer" in query_lower or "user" in query_lower:
            return "customer analysis"
        elif "order" in query_lower or "sale" in query_lower:
            return "sales performance"
        elif "product" in query_lower:
            return "product insights"
        elif "campaign" in query_lower or "marketing" in query_lower:
            return "marketing effectiveness"
        elif "growth" in query_lower:
            return "growth opportunities"
        elif "performance" in query_lower:
            return "performance metrics"
        else:
            return "business insights"

    def _generate_customer_insight(self, metrics: dict, query: str) -> str:
        """Generate customer-focused insights based on actual data"""
        users = metrics.get("users", 0)
        orders = metrics.get("orders", 0)
        activities = metrics.get("activities", 0)
        
        if users and orders:
            ratio = round(orders / users, 1)
            if ratio < 1.5:
                return f"Your {users:,} users have generated {orders:,} orders ({ratio} per user), which suggests significant opportunity to increase customer engagement and repeat purchases. Focus on understanding what motivates your most active customers and applying those insights to boost overall engagement."
            else:
                return f"Your {users:,} users averaging {ratio} orders each shows good customer engagement. The opportunity now is optimizing the customer journey and potentially increasing order values or introducing complementary products."
        elif users:
            return f"With {users:,} users in your system, understanding their behavior patterns and preferences will be key to driving growth and improving customer satisfaction."
        else:
            return "Customer insights will be valuable for understanding preferences, behavior patterns, and growth opportunities once we access the relevant data."

    def _generate_growth_insight(self, metrics: dict, query: str) -> str:
        """Generate growth-focused insights based on actual data"""
        users = metrics.get("users", 0)
        orders = metrics.get("orders", 0)
        products = metrics.get("products", 0)
        
        insights = []
        if users and orders:
            ratio = orders / users
            if ratio < 2:
                insights.append("increasing customer purchase frequency")
            else:
                insights.append("scaling successful customer segments")
        
        if products:
            insights.append(f"optimizing your {products:,} product mix")
        
        if users:
            insights.append("expanding your customer acquisition strategies")
        
        if insights:
            return f"your growth opportunities likely include {', '.join(insights)}. The key is identifying what's already working well and scaling those successful patterns."
        else:
            return "growth opportunities will become clear once we analyze your customer patterns, sales trends, and operational efficiency metrics."

    def _generate_performance_insight(self, metrics: dict, query: str) -> str:
        """Generate performance-focused insights based on actual data"""
        total_entities = sum(metrics.values())
        
        if total_entities > 20000:
            return f"you're operating at significant scale with strong data collection. Performance optimization should focus on identifying your highest-impact activities and scaling them effectively."
        elif total_entities > 5000:
            return f"you have solid operational scale with good data depth. Performance improvements likely come from understanding which activities drive the best results and doing more of those."
        else:
            return f"you have a solid foundation for growth. Performance gains will come from understanding your most effective strategies and systematically improving them."

    def _generate_general_insight(self, metrics: dict, query: str) -> str:
        """Generate general business insights based on actual data"""
        total_entities = sum(metrics.values())
        entity_types = len(metrics)
        
        if entity_types >= 3:
            return f"you have comprehensive business data across {entity_types} key areas. This gives you the ability to understand connections between different aspects of your business and make more informed strategic decisions."
        elif total_entities > 10000:
            return f"your business generates substantial data, which provides excellent opportunities for understanding patterns, trends, and optimization opportunities."
        else:
            return f"there are clear opportunities to leverage your business data for better decision-making and strategic planning."

    async def _generate_contextual_fallback(self, query: str, result: dict) -> str:
        """Generate contextual fallback response"""
        try:
            # Even fallbacks should be helpful and contextual
            if "customer" in query.lower():
                return "Customer insights are incredibly valuable for business growth. I can help you understand customer behavior patterns, preferences, and opportunities once I access the relevant data. What specific aspect of your customer base interests you most?"
            elif "growth" in query.lower():
                return "Growth opportunities often hide in existing data patterns. I can help you identify what's working well and how to scale it, plus spot new opportunities you might not have considered. What type of growth are you most focused on?"
            elif "performance" in query.lower():
                return "Performance analysis can reveal both quick wins and strategic opportunities. I can help you understand which metrics matter most and how to improve them systematically. Which area of performance concerns you most?"
            else:
                return "That's a great question that would benefit from analyzing your actual business data. I can provide specific insights and recommendations once I access the relevant information. What would be most helpful for your immediate needs?"
        except Exception as e:
            logger.error(f"Error in fallback generation: {e}")
            return "I'm here to help you get meaningful insights from your business data. What would you like to explore?"

    def _understand_human_intent(self, query: str) -> dict:
        """Understand what the human really wants to know"""
        query_lower = query.lower()
        
        intent = {
            "primary_goal": "understand",
            "emotion": "neutral",
            "urgency": "normal",
            "business_focus": "general",
            "communication_style": "professional"
        }
        
        # Detect primary goals
        if any(word in query_lower for word in ["help", "how can", "what should", "recommend", "suggest"]):
            intent["primary_goal"] = "get_advice"
        elif any(word in query_lower for word in ["analyze", "insights", "patterns", "trends", "understand"]):
            intent["primary_goal"] = "get_insights"
        elif any(word in query_lower for word in ["performance", "metrics", "kpi", "results", "numbers"]):
            intent["primary_goal"] = "see_performance"
        elif any(word in query_lower for word in ["growth", "improve", "optimize", "better", "increase"]):
            intent["primary_goal"] = "drive_growth"
        elif any(word in query_lower for word in ["transform", "change", "strategy", "future"]):
            intent["primary_goal"] = "strategic_planning"
        
        # Detect emotional tone
        if any(word in query_lower for word in ["urgent", "quickly", "asap", "immediately"]):
            intent["urgency"] = "high"
            intent["emotion"] = "urgent"
        elif any(word in query_lower for word in ["struggling", "problem", "issue", "challenge"]):
            intent["emotion"] = "concerned"
        elif any(word in query_lower for word in ["excited", "opportunity", "potential"]):
            intent["emotion"] = "optimistic"
        
        # Business focus area
        if any(word in query_lower for word in ["customer", "user", "client"]):
            intent["business_focus"] = "customer"
        elif any(word in query_lower for word in ["marketing", "campaign", "advertising"]):
            intent["business_focus"] = "marketing"
        elif any(word in query_lower for word in ["sales", "revenue", "profit"]):
            intent["business_focus"] = "sales"
        elif any(word in query_lower for word in ["operation", "process", "efficiency"]):
            intent["business_focus"] = "operations"
        
        return intent

    def _extract_data_context(self, result: dict) -> dict:
        """Extract meaningful business context from the data"""
        context = {
            "data_available": False,
            "total_records": 0,
            "collections": [],
            "business_areas": [],
            "key_metrics": {}
        }
        
        try:
            if isinstance(result, dict) and "results" in result:
                results_data = result["results"]
                
                if isinstance(results_data, dict) and "load_schema" in results_data:
                    schema_data = results_data["load_schema"]
                    
                    if isinstance(schema_data, dict) and "collections" in schema_data:
                        collections = schema_data["collections"]
                        context["data_available"] = True
                        context["collections"] = collections
                        
                        # Calculate meaningful metrics
                        total_records = 0
                        business_areas = []
                        
                        for col in collections:
                            if isinstance(col, dict):
                                count = col.get("count", 0)
                                total_records += count
                                name = col.get("name", "").lower()
                                
                                # Map collections to business areas
                                if any(keyword in name for keyword in ["user", "customer", "client"]):
                                    business_areas.append("customer_management")
                                    context["key_metrics"]["customers"] = count
                                elif any(keyword in name for keyword in ["order", "purchase", "transaction"]):
                                    business_areas.append("sales")
                                    context["key_metrics"]["transactions"] = count
                                elif any(keyword in name for keyword in ["campaign", "marketing", "email"]):
                                    business_areas.append("marketing")
                                    context["key_metrics"]["campaigns"] = count
                                elif any(keyword in name for keyword in ["product", "inventory", "item"]):
                                    business_areas.append("product_management")
                                    context["key_metrics"]["products"] = count
                        
                        context["total_records"] = total_records
                        context["business_areas"] = list(set(business_areas))
        
        except Exception as e:
            logger.error(f"Error extracting data context: {e}")
        
        return context

    def _craft_natural_response(self, query: str, intent: dict, data_context: dict) -> str:
        """Create natural, conversational responses based on intent and context"""
        
        # Start with context awareness
        if not data_context["data_available"]:
            return self._generate_no_data_response(query, intent)
        
        # Generate response based on primary goal
        if intent["primary_goal"] == "get_advice":
            return self._generate_advice_response(query, intent, data_context)
        elif intent["primary_goal"] == "get_insights":
            return self._generate_insights_response(query, intent, data_context)
        elif intent["primary_goal"] == "see_performance":
            return self._generate_performance_response(query, intent, data_context)
        elif intent["primary_goal"] == "drive_growth":
            return self._generate_growth_response(query, intent, data_context)
        elif intent["primary_goal"] == "strategic_planning":
            return self._generate_strategy_response(query, intent, data_context)
        else:
            return self._generate_general_response(query, intent, data_context)

    def _generate_advice_response(self, query: str, intent: dict, data_context: dict) -> str:
        """Generate natural advice with strategic insight flowing into practical implementation"""
        total_records = data_context["total_records"]
        key_metrics = data_context["key_metrics"]
        
        # Strategic advice with practical implementation guidance
        if "grow" in query.lower() and ("10x" in query.lower() or "transform" in query.lower()):
            if "customers" in key_metrics and "transactions" in key_metrics:
                customer_count = key_metrics["customers"]
                transaction_count = key_metrics["transactions"]
                avg_transactions = transaction_count / customer_count if customer_count > 0 else 0
                
                if avg_transactions < 2:
                    return f"Your {customer_count:,} customers with {transaction_count:,} purchases reveal a powerful 10x growth opportunity - most customers are buying only once, which means there's enormous untapped potential in your existing base. The strategic insight here is that getting existing customers to buy again is typically far more profitable than acquiring new ones. Here's how to approach this transformation: Focus on understanding what makes your repeat customers different and then systematically apply those insights to encourage similar behavior in your single-purchase customers. Develop structured follow-up strategies that provide ongoing value and build relationships over time. Create engagement timelines that keep your brand relevant and helpful rather than pushy. The key is building a systematic approach to customer development that turns one-time buyers into loyal, repeat customers. You'll see transformation when your purchase frequency moves from {avg_transactions:.1f} toward 2.5 or higher - that change alone could triple your revenue from existing customers."
                else:
                    return f"With {customer_count:,} customers averaging {avg_transactions:.1f} purchases each, you already have strong customer engagement - the foundation for 10x growth. The strategic opportunity lies in two areas: attracting more customers like your best ones and increasing the value each customer generates. Here's your systematic approach: First, deeply understand your highest-value customers - where did you find them, what do they buy, how do they engage with your business? Then invest heavily in the channels and strategies that attract similar customers. Simultaneously, analyze what drives your largest transactions and develop approaches to encourage higher-value purchases across your customer base. Focus on building systematic processes for customer acquisition, engagement, and value development. The transformation happens when you combine scaled customer acquisition with increased customer value - both feeding into exponential rather than linear growth."
            else:
                return f"Your foundation of {total_records:,} data points provides the intelligence needed for 10x transformation. The key strategic insight is that exponential growth comes from finding what's already working and scaling it aggressively while building new capabilities systematically. Here's how to approach transformation: Identify your highest-performing business activities and understand what makes them successful. Then develop systematic approaches to replicate and scale these successes while experimenting with new growth strategies. Focus on building competitive advantages through better customer relationships, more effective operations, and superior market positioning. Create structured approaches to innovation, testing, and scaling that can support exponential rather than incremental growth."
        
        elif "customers" in query.lower() and "want" in query.lower():
            if "customers" in key_metrics:
                customer_count = key_metrics["customers"]
                if "transactions" in key_metrics:
                    transaction_count = key_metrics["transactions"]
                    return f"Understanding what your {customer_count:,} customers truly want requires looking beyond what they say to what they actually do - and your {transaction_count:,} transactions provide that behavioral intelligence. The strategic insight is that customer actions reveal preferences more accurately than surveys or feedback. Here's how to decode their true needs: Study the patterns in repeat customer behavior - what they buy, when they buy it, and what triggers their purchases. Look for commonalities among your most engaged customers and differences between high-value and low-value segments. Pay attention to seasonal patterns, purchase sequences, and engagement timing. The key is developing systematic approaches to understand customer behavior and translate those insights into better products, services, and experiences. Focus on building ongoing feedback loops through customer interactions, purchase analysis, and engagement monitoring that help you stay aligned with evolving customer needs."
                else:
                    return f"With {customer_count:,} customers in your database, you have significant opportunity to understand and respond to customer needs more effectively. The strategic approach combines behavioral analysis with direct feedback to build a comprehensive understanding of what drives customer satisfaction and loyalty. Develop systematic approaches to gathering customer insights through their interactions, preferences, and feedback. Focus on identifying patterns that indicate what customers value most and what experiences lead to long-term relationships. Use these insights to guide product development, service improvements, and customer experience optimization."
            else:
                return f"Customer understanding with {total_records:,} data points requires systematic approaches to connecting customer behavior with business outcomes. The key is developing ongoing intelligence about what drives customer satisfaction, loyalty, and value. Focus on building feedback systems that help you understand customer needs, preferences, and experiences. Use behavioral data to complement direct feedback and create comprehensive customer insights that guide business strategy and operational improvements."
        
        else:
            # General strategic business advice with implementation guidance
            if "customers" in key_metrics and key_metrics["customers"] > 1000:
                return f"With {key_metrics['customers']:,} customers, you have a solid foundation for strategic growth. The key insight is that sustainable growth comes from understanding your existing customers deeply and building systematic approaches to serve them better while attracting similar high-value customers. Here's how to approach sustainable growth: Focus first on maximizing value from existing relationships through better service, more relevant offerings, and stronger engagement. Then use what you learn about your best customers to guide acquisition strategies and business development. Build systematic processes for customer development, performance measurement, and continuous improvement. The goal is creating sustainable competitive advantages through superior customer relationships and more effective operations that compound over time."
            else:
                return f"Your business foundation with {total_records:,} data points provides the intelligence needed for strategic development. The key is building systematic approaches to customer understanding, performance optimization, and sustainable growth. Focus on creating strong unit economics, understanding customer value drivers, and building scalable processes that can support growth over time. Develop metrics and feedback systems that help you make data-driven decisions and continuously improve your business performance."

    def _generate_insights_response(self, query: str, intent: dict, data_context: dict) -> str:
        """Generate natural insights analysis"""
        key_metrics = data_context["key_metrics"]
        business_areas = data_context["business_areas"]
        
        if "customer" in query.lower() and "behavior" in query.lower():
            if "customers" in key_metrics and "transactions" in key_metrics:
                customer_count = key_metrics["customers"]
                transaction_count = key_metrics["transactions"]
                avg_transactions = transaction_count / customer_count if customer_count > 0 else 0
                
                return f"Your customer behavior data tells an interesting story. With {customer_count:,} customers generating {transaction_count:,} transactions, you're seeing {avg_transactions:.1f} purchases per customer on average. What's particularly revealing is the distribution - you likely have a small group of highly engaged customers driving a disproportionate amount of your revenue, while a larger group has only purchased once. The key insight here is that your repeat customers are showing you the path forward. They've found something valuable enough to come back for, and understanding what that is will help you create that same value for your one-time buyers."
            else:
                return f"Customer behavior patterns are hidden in the data you're already collecting. The most valuable insights come from comparing your most engaged customers with your least engaged ones. Look at the customer journey - what did your best customers experience that others didn't? That's where you'll find your growth opportunities."
        
        elif "pattern" in query.lower() or "trend" in query.lower():
            if "marketing" in business_areas and "sales" in business_areas:
                return f"The patterns I'm seeing across your marketing and sales data suggest there are clear connections between campaign performance and customer acquisition quality. Some channels are probably bringing in customers who stick around and buy more, while others are bringing in one-time buyers. The trend to watch is customer lifetime value by acquisition source - that's where you'll find your most actionable insights."
            else:
                return f"Pattern recognition in your data can reveal opportunities that aren't obvious at first glance. With {data_context['total_records']:,} data points, you have enough information to identify seasonal trends, customer lifecycle patterns, and performance correlations. The most valuable patterns are usually the ones that connect customer behavior to business outcomes."
        
        else:
            # General insights
            if len(business_areas) >= 2:
                return f"What stands out most in your data is the breadth of information you're capturing across {len(business_areas)} different business areas. This gives you the ability to see connections that many businesses miss - like how marketing campaign performance correlates with customer satisfaction, or how product preferences vary by customer acquisition channel. The real insight opportunity is in connecting these different data streams to understand the full customer journey."
            else:
                return f"Your data foundation of {data_context['total_records']:,} records provides a solid base for understanding your business patterns. The key is looking beyond surface-level metrics to understand cause and effect relationships. What drives customer retention? Which factors predict higher transaction values? Those deeper insights are what turn data into competitive advantage."

    def _generate_performance_response(self, query: str, intent: dict, data_context: dict) -> str:
        """Generate natural performance analysis"""
        key_metrics = data_context["key_metrics"]
        
        if not key_metrics:
            return f"Your system is tracking {data_context['total_records']:,} data points across {len(data_context['collections'])} areas. To give you specific performance insights, I'd need to analyze trends over time and benchmark against your goals."
        
        performance_notes = []
        
        # Customer performance
        if "customers" in key_metrics:
            customer_count = key_metrics["customers"]
            if "transactions" in key_metrics:
                transaction_count = key_metrics["transactions"]
                ratio = transaction_count / customer_count if customer_count > 0 else 0
                if ratio > 3:
                    performance_notes.append(f"Strong customer engagement with {ratio:.1f} transactions per customer.")
                elif ratio > 1.5:
                    performance_notes.append(f"Good customer activity at {ratio:.1f} transactions per customer, with room to increase frequency.")
                else:
                    performance_notes.append(f"Customer engagement at {ratio:.1f} transactions per customer suggests opportunities to boost repeat purchases.")
        
        # Marketing performance
        if "campaigns" in key_metrics:
            campaign_count = key_metrics["campaigns"]
            if "customers" in key_metrics:
                campaigns_per_customer = campaign_count / key_metrics["customers"] if key_metrics["customers"] > 0 else 0
                if campaigns_per_customer > 0.1:
                    performance_notes.append(f"Active marketing approach with multiple touchpoints per customer.")
                else:
                    performance_notes.append(f"Conservative marketing approach with opportunities to increase customer touchpoints.")
        
        if performance_notes:
            return " ".join(performance_notes)
        else:
            return f"Your operations are generating substantial data with {data_context['total_records']:,} records. The breadth across {len(data_context['collections'])} areas indicates comprehensive business tracking."

    def _generate_growth_response(self, query: str, intent: dict, data_context: dict) -> str:
        """Generate natural growth strategy recommendations"""
        key_metrics = data_context["key_metrics"]
        business_areas = data_context["business_areas"]
        
        growth_strategies = []
        
        # Customer growth strategies
        if "customer_management" in business_areas:
            customer_count = key_metrics.get("customers", 0)
            if "sales" in business_areas:
                transaction_count = key_metrics.get("transactions", 0)
                if customer_count > 0 and transaction_count > 0:
                    avg_value = transaction_count / customer_count
                    if avg_value < 2:
                        growth_strategies.append("Focus on increasing purchase frequency - your customers have room to buy more often.")
                    else:
                        growth_strategies.append("With strong repeat purchase patterns, expand into new customer segments or increase order values.")
            else:
                growth_strategies.append(f"Leverage your {customer_count:,} customer base by implementing referral programs and expanding into adjacent markets.")
        
        # Marketing growth
        if "marketing" in business_areas and "campaigns" in key_metrics:
            campaign_count = key_metrics["campaigns"]
            growth_strategies.append(f"Scale your marketing by testing new channels - you have {campaign_count} campaigns to analyze for what works best.")
        
        # Data-driven growth
        total_records = data_context["total_records"]
        if total_records > 10000:
            growth_strategies.append("Use predictive analytics on your data to identify high-value opportunities before competitors do.")
        
        if growth_strategies:
            return " ".join(growth_strategies)
        else:
            return f"With {data_context['total_records']:,} data points to work with, focus on identifying your highest-performing segments and double down on what's already working while testing scaled versions in new markets."

    def _generate_strategy_response(self, query: str, intent: dict, data_context: dict) -> str:
        """Generate natural strategic planning insights"""
        business_areas = data_context["business_areas"]
        key_metrics = data_context["key_metrics"]
        
        strategic_points = []
        
        # Strategic positioning based on data assets
        if len(business_areas) >= 3:
            strategic_points.append("You have comprehensive data across multiple business functions, which gives you a competitive advantage in making integrated decisions.")
        
        # Customer strategy
        if "customer_management" in business_areas and "sales" in business_areas:
            customer_count = key_metrics.get("customers", 0)
            transaction_count = key_metrics.get("transactions", 0)
            if customer_count > 0 and transaction_count > 0:
                strategic_points.append(f"Your customer base of {customer_count:,} with {transaction_count:,} transactions provides a strong foundation for building predictable revenue streams.")
        
        # Market positioning
        if "marketing" in business_areas:
            strategic_points.append("Your marketing data can inform long-term brand positioning and help you identify which customer segments offer the highest lifetime value.")
        
        # Future readiness
        total_records = data_context["total_records"]
        strategic_points.append(f"The scale of your data ({total_records:,} records) positions you well for AI-driven automation and predictive business models.")
        
        if strategic_points:
            return " ".join(strategic_points)
        else:
            return f"Your strategic advantage lies in the breadth of data you're collecting. Focus on connecting insights across different business areas to identify opportunities that aren't visible when looking at each area in isolation."

    def _generate_general_response(self, query: str, intent: dict, data_context: dict) -> str:
        """Generate natural general responses"""
        total_records = data_context["total_records"]
        collections_count = len(data_context["collections"])
        key_metrics = data_context["key_metrics"]
        
        # Start with context
        opening = f"Looking at your business data, you have {total_records:,} records across {collections_count} different areas."
        
        # Add relevant insights
        insights = []
        if key_metrics:
            if "customers" in key_metrics:
                insights.append(f"{key_metrics['customers']:,} customers")
            if "transactions" in key_metrics:
                insights.append(f"{key_metrics['transactions']:,} transactions")
            if "campaigns" in key_metrics:
                insights.append(f"{key_metrics['campaigns']} marketing campaigns")
        
        if insights:
            middle = f" This includes {', '.join(insights)}."
        else:
            middle = f" The data spans multiple operational areas with significant depth."
        
        # End with actionable direction
        ending = " What specific aspect would you like me to analyze deeper?"
        
        return opening + middle + ending

    def _generate_no_data_response(self, query: str, intent: dict) -> str:
        """Generate response when no data is available"""
        if intent["urgency"] == "high":
            return "I'd be happy to help you with that analysis. Could you provide access to the relevant data, or let me know which systems we should connect to get the insights you need?"
        else:
            return "To give you specific insights on that, I'll need access to your business data. Once connected, I can provide detailed analysis and recommendations based on your actual metrics and trends."

    def _generate_conversational_fallback(self, query: str) -> str:
        """Natural fallback when other methods fail"""
        if "help" in query.lower():
            return "I'm here to help you understand your business better. What specific area would you like to explore - customers, sales, marketing, or operations?"
        elif "performance" in query.lower():
            return "I can analyze performance across different areas of your business. Which metrics or time period are you most interested in?"
        else:
            return "That's an interesting question. To give you the most useful insights, could you tell me a bit more about what you're trying to achieve?"

    def _extract_ai_insights(self, result: dict) -> dict:
        """Extract actual insights and analysis from AI agents"""
        insights = {
            "understanding": "",
            "key_findings": [],
            "recommendations": [],
            "data_points": {},
            "confidence": 0.0
        }
        
        try:
            logger.info(f"ðŸ§  EXTRACTING AI INSIGHTS from result keys: {list(result.keys()) if isinstance(result, dict) else 'Not a dict'}")
            
            # Since the agent reasoning isn't being captured in messages properly,
            # let's work with the available schema data and create intelligent insights
            
            if isinstance(result, dict):
                # Extract schema data for context
                if "results" in result and result["results"]:
                    results_data = result["results"]
                    logger.info(f"ðŸ§  Found results data with keys: {list(results_data.keys()) if isinstance(results_data, dict) else 'Not a dict'}")
                    
                    if isinstance(results_data, dict):
                        insights["data_points"].update(results_data)
                        
                        # Extract schema insights for intelligent recommendations
                        if "load_schema" in results_data:
                            schema_data = results_data["load_schema"]
                            if isinstance(schema_data, dict) and "collections" in schema_data:
                                collections = schema_data["collections"]
                                total_records = sum(col.get("count", 0) for col in collections if isinstance(col, dict))
                                
                                insights["understanding"] = "Business growth analysis requiring comprehensive data review across operational areas"
                                insights["key_findings"].append(f"Data foundation: {total_records:,} records across {len(collections)} collections available for analysis")
                                insights["confidence"] = 0.8
                                
                                # Generate specific insights based on available collections
                                for col in collections:
                                    if isinstance(col, dict):
                                        name = col.get("name", "")
                                        count = col.get("count", 0)
                                        
                                        if name == "users" and count > 0:
                                            insights["key_findings"].append(f"Customer database: {count:,} user records indicate strong customer base for growth analysis")
                                            insights["recommendations"].append(f"Analyze customer segmentation and behavior patterns from {count:,} user records to identify growth opportunities")
                                            
                                        elif name == "orders" and count > 0:
                                            insights["key_findings"].append(f"Sales history: {count:,} order records provide transaction pattern insights")
                                            insights["recommendations"].append(f"Review purchase frequency and order value trends across {count:,} transactions to optimize revenue")
                                            
                                        elif name == "products" and count > 0:
                                            insights["key_findings"].append(f"Product catalog: {count:,} product records enable product performance analysis")
                                            insights["recommendations"].append(f"Identify top-performing products from {count:,} catalog items and focus marketing efforts")
                                            
                                        elif name == "marketing_campaigns" and count > 0:
                                            insights["key_findings"].append(f"Marketing data: {count:,} campaign records available for effectiveness analysis")
                                            insights["recommendations"].append(f"Optimize marketing spend by analyzing performance across {count:,} campaign records")
                                            
                                        elif name == "user_activity" and count > 0:
                                            insights["key_findings"].append(f"User engagement: {count:,} activity records show customer interaction patterns")
                                            insights["recommendations"].append(f"Improve user experience based on engagement patterns from {count:,} activity records")
                
                # Add intelligent business growth recommendations
                if not insights["recommendations"]:
                    insights["recommendations"] = [
                        "Focus on customer retention by analyzing user behavior and engagement patterns",
                        "Optimize product mix based on sales performance and demand analysis", 
                        "Improve marketing ROI by identifying highest-converting campaigns and channels",
                        "Enhance operational efficiency through data-driven process optimization"
                    ]
                
                if not insights["key_findings"]:
                    insights["key_findings"] = [
                        "Comprehensive business data ecosystem available for strategic analysis",
                        "Multiple data sources enable cross-functional business intelligence",
                        "Integration opportunities exist for enhanced customer insights"
                    ]
            
            logger.info(f"ðŸ§  EXTRACTED: understanding={bool(insights['understanding'])}, findings={len(insights['key_findings'])}, recs={len(insights['recommendations'])}")
            
            # Return insights if we have meaningful content
            if insights["key_findings"] or insights["recommendations"]:
                return insights
            else:
                return None
                
        except Exception as e:
            logger.error(f"Error extracting AI insights: {e}")
            return None

    def _generate_intelligent_response(self, query: str, insights: dict, result: dict) -> str:
        """Generate intelligent response based on actual AI analysis"""
        try:
            # ALL responses now go through the new natural conversation system
            return self._craft_natural_response_sync(query, result)
        except Exception as e:
            logger.error(f"Error generating intelligent response: {e}")
            return "I can see there's valuable information in your data that could help answer your question. Let me analyze this from a different angle to provide you with the most useful insights."

    def _craft_natural_response_sync(self, query: str, result: dict) -> str:
        """Synchronous version of natural response generation with strategic insights and practical implementation"""
        try:
            # Understand what the human is really asking
            intent = self._understand_human_intent(query)
            
            # Extract relevant data context
            data_context = self._extract_data_context(result)
            
            # Generate natural, conversational response with strategic insight + practical implementation
            if intent["type"] == "business_advice":
                if "customers" in data_context:
                    customers = data_context['customers']
                    transactions = data_context.get('transactions', 0)
                    ratio = transactions / customers if customers > 0 else 0
                    
                    if "retention" in query.lower():
                        return f"Your {customers:,} customers generating {transactions:,} transactions shows a {ratio:.1f} purchase frequency - this reveals a significant retention opportunity. Most businesses see their biggest growth when they increase repeat purchases rather than just acquiring new customers. Here's how to approach this systematically: Start by understanding what makes your current repeat customers different - study their purchase timing, preferences, and engagement patterns. Then create targeted follow-up strategies for single-purchase customers based on these successful patterns. Develop a structured communication timeline that keeps your brand relevant without being pushy. Focus on providing value through helpful content, exclusive offers, and personalized recommendations based on their initial purchase. The key is building relationships that naturally lead to repeat business rather than pushing for immediate sales. You'll know this is working when you see your purchase frequency steadily increase from {ratio:.1f} toward 2.0 or higher over the next few months."
                    else:
                        return f"Looking at your business data - {customers:,} customers with {transactions:,} transactions ({ratio:.1f} per customer) - the strategic opportunity is clear: you have solid customer acquisition but significant room for improved customer lifetime value. The most effective approach combines understanding your best customers with systematic engagement of your broader base. Begin by identifying the characteristics and behaviors of customers who purchase multiple times, then develop strategies to encourage similar behavior in your single-purchase customers. Create a systematic follow-up approach that provides ongoing value - whether through helpful content, exclusive access, or personalized recommendations. Build relationships that naturally evolve into repeat business rather than just pushing for immediate sales. Implement a structured timeline for customer engagement and track your progress by monitoring how your purchase frequency improves from {ratio:.1f} toward higher levels over time."
                else:
                    return "Your business data reveals strong fundamentals with clear opportunities for optimization. The key is focusing on high-impact activities that drive sustainable growth. Start by identifying your most successful business activities and understanding what makes them work so well. Then develop systematic approaches to replicate and scale these successes. Create structured processes for engaging with customers, measuring results, and continuously improving based on what you learn. Focus on building sustainable competitive advantages through better customer relationships and more effective operations. Track your progress with meaningful metrics that reflect real business health and growth potential."
            
            elif intent["type"] == "data_analysis":
                if data_context.get("total_collections", 0) > 0:
                    collections = data_context.get("total_collections", 0)
                    records = data_context.get("total_records", 0)
                    return f"Your database with {collections} collections and {records:,} records contains valuable strategic intelligence waiting to be unlocked. The key insight is understanding the relationships between different aspects of your business - how customer behavior connects to revenue patterns, how engagement levels predict future value, and how different business activities influence outcomes. Here's how to extract actionable insights: Begin by mapping the connections between your customer data and business results to identify your most valuable patterns. Look for trends in customer behavior that predict success, seasonal patterns that affect performance, and segments that show different characteristics. Develop a systematic approach to analyzing these patterns regularly so you can make data-driven decisions rather than guessing. Focus on understanding causation, not just correlation - what actually drives the results you want? Use these insights to guide strategy, resource allocation, and operational improvements for sustainable business growth."
                else:
                    return "Your data structure reveals multiple interconnected business processes with significant analytical potential. The strategic opportunity lies in understanding how different elements of your business influence each other and drive overall performance. Develop systematic approaches to analyze customer behavior, business performance, and operational efficiency. Look for patterns that indicate what's working well and what needs improvement. Create regular review processes that help you understand trends, identify opportunities, and make informed decisions about resource allocation and strategic direction."
            
            elif intent["type"] == "performance_metrics":
                return "Effective performance management combines the right metrics with systematic improvement processes. The key is focusing on indicators that actually predict business success rather than just reporting what happened. Start by identifying the key drivers of your business results - usually customer satisfaction, operational efficiency, and revenue quality metrics. Develop systematic approaches to measuring and improving these drivers over time. Create regular review cycles that help you understand trends, identify problems early, and recognize opportunities for improvement. Focus on leading indicators that help you make proactive decisions rather than just reactive responses. Build sustainable processes for continuous improvement based on performance insights."
            
            else:
                # General conversational response with strategic + implementation guidance
                return "Your business situation shows solid fundamentals with clear opportunities for strategic improvement. The most effective approach combines understanding what's currently working well with systematic development of new capabilities. Start by analyzing your strongest business activities to understand what makes them successful, then develop strategies to replicate and scale these successes. Build systematic processes for customer engagement, performance measurement, and continuous improvement. Focus on creating sustainable competitive advantages through better customer relationships, more effective operations, and data-driven decision making. Track your progress with meaningful metrics and adjust your approach based on results and changing business conditions."
                
        except Exception as e:
            logger.error(f"Error in natural response generation: {e}")
            return "Your business data contains valuable strategic insights that can guide meaningful improvements. The key is developing systematic approaches to understand what's working well and building on those strengths while addressing areas that need enhancement. Focus on sustainable strategies that create long-term value for both your business and your customers."

    # Override any remaining old hardcoded methods to prevent templated responses
    def _generate_advice_from_insights(self, *args, **kwargs):
        """Redirect old method calls to natural system"""
        return self._craft_natural_response_sync(args[0] if args else "general query", args[2] if len(args) > 2 else {})
    
    def _generate_analysis_from_insights(self, *args, **kwargs):
        """Redirect old method calls to natural system"""
        return self._craft_natural_response_sync(args[0] if args else "analysis query", args[2] if len(args) > 2 else {})
    
    def _generate_performance_from_insights(self, *args, **kwargs):
        """Redirect old method calls to natural system"""
        return self._craft_natural_response_sync(args[0] if args else "performance query", args[2] if len(args) > 2 else {})
    
    def _generate_general_from_insights(self, *args, **kwargs):
        """Redirect old method calls to natural system"""
        return self._craft_natural_response_sync(args[0] if args else "general query", args[2] if len(args) > 2 else {})
    
    def _generate_contextual_business_advice(self, *args, **kwargs):
        """Redirect old method calls to natural system"""
        return self._craft_natural_response_sync(args[0] if args else "business advice", args[2] if len(args) > 2 else {})

    def _parse_ai_understanding(self, insights: dict) -> dict:
        """Parse the AI insights to extract key points for response generation"""  
        key_points = {
            "intent": "",
            "complexity": "",
            "data_sources": [],
            "analysis_steps": [],
            "findings": [],
            "recommendations": []
        }
        
        try:
            # Extract from the insights dict structure
            if isinstance(insights, dict):
                # Get understanding/intent
                if "understanding" in insights:
                    key_points["intent"] = insights["understanding"]
                
                # Get findings
                if "key_findings" in insights:
                    key_points["findings"] = insights["key_findings"][:5]  # Limit to top 5
                
                # Get recommendations
                if "recommendations" in insights:
                    key_points["recommendations"] = insights["recommendations"][:5]  # Limit to top 5
                
                # Set complexity based on confidence
                confidence = insights.get("confidence", 0.0)
                if confidence > 0.8:
                    key_points["complexity"] = "high"
                elif confidence > 0.5:
                    key_points["complexity"] = "medium"
                else:
                    key_points["complexity"] = "low"
                
                # Extract data sources from findings and recommendations
                data_sources = set()
                all_text = " ".join(key_points["findings"] + key_points["recommendations"])
                for source in ["users", "orders", "products", "customers", "sales", "marketing", "campaigns", "support", "warehouses"]:
                    if source in all_text.lower():
                        data_sources.add(source)
                key_points["data_sources"] = list(data_sources)
                
            return key_points
            
        except Exception as e:
            logger.error(f"Error parsing AI understanding: {e}")
            return key_points
            
        try:
            # Extract intent
            if "intent" in understanding.lower():
                lines = understanding.split('\n')
                for line in lines:
                    if "intent" in line.lower() and ":" in line:
                        key_points["intent"] = line.split(':', 1)[1].strip()
                        break
            
            # Extract complexity assessment
            if "complexity" in understanding.lower():
                if "high" in understanding.lower():
                    key_points["complexity"] = "high"
                elif "medium" in understanding.lower():
                    key_points["complexity"] = "medium"
                else:
                    key_points["complexity"] = "low"
            
            # Extract data sources mentioned
            collections_mentioned = []
            for collection in ["user_activity", "orders", "products", "marketing_campaigns", "warehouses", "shipments", "users", "employees", "support_tickets", "digital_content"]:
                if collection in understanding.lower():
                    collections_mentioned.append(collection)
            key_points["data_sources"] = collections_mentioned
            
            # Extract findings and recommendations from AI analysis
            lines = understanding.split('\n')
            in_recommendations = False
            in_findings = False
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                if "recommendation" in line.lower() or "suggest" in line.lower():
                    in_recommendations = True
                    in_findings = False
                elif "finding" in line.lower() or "insight" in line.lower():
                    in_findings = True
                    in_recommendations = False
                elif line.startswith('*') or line.startswith('-') or line.startswith('â€¢'):
                    if in_recommendations:
                        key_points["recommendations"].append(line[1:].strip())
                    elif in_findings:
                        key_points["findings"].append(line[1:].strip())
                        
            return key_points
            
        except Exception as e:
            logger.error(f"Error parsing AI understanding: {e}")
            return key_points

    def _generate_performance_from_insights(self, query: str, key_points: dict, data_points: dict) -> str:
        """Generate intelligent performance insights based on actual metrics and AI analysis"""
        try:
            # Extract AI insights
            understanding = key_points.get("intent", "")
            
            # Get actual performance metrics from data
            performance_insights = self._calculate_performance_metrics(data_points)
            
            response_parts = []
            
            # Present key performance indicators directly
            if performance_insights["customer_performance"]:
                response_parts.append(performance_insights["customer_performance"])
            
            if performance_insights["operational_efficiency"]:
                response_parts.append(performance_insights["operational_efficiency"])
            
            if performance_insights["marketing_effectiveness"]:
                response_parts.append(performance_insights["marketing_effectiveness"])
            
            # Add specific improvement recommendations
            if performance_insights["improvement_areas"]:
                response_parts.extend(performance_insights["improvement_areas"][:2])
            
            return "\n\n".join(response_parts) if response_parts else "Performance metrics indicate balanced operations with opportunities for strategic optimization across customer engagement and operational efficiency."
            
        except Exception as e:
            logger.error(f"Error generating performance insights: {e}")
            return "Performance analysis reveals optimization opportunities across customer engagement, operational efficiency, and marketing effectiveness."

    def _calculate_performance_metrics(self, data_points: dict) -> dict:
        """Calculate actual performance metrics from data"""
        metrics = {
            "customer_performance": "",
            "operational_efficiency": "",
            "marketing_effectiveness": "",
            "improvement_areas": []
        }
        
        try:
            if "schema" in data_points and "collections" in data_points["schema"]:
                collections = data_points["schema"]["collections"]
                
                # Extract key counts
                counts = {}
                for col in collections:
                    name = col.get("name", "")
                    count = col.get("count", 0)
                    counts[name] = count
                
                # Calculate customer performance
                if "orders" in counts and "users" in counts and counts["users"] > 0:
                    orders_per_user = counts["orders"] / counts["users"]
                    metrics["customer_performance"] = f"**Customer Efficiency:** {orders_per_user:.1f} orders per customer"
                    
                    if orders_per_user < 2:
                        metrics["improvement_areas"].append("**Priority:** Increase customer lifetime value - implement loyalty programs and personalized offers")
                    elif orders_per_user > 4:
                        metrics["improvement_areas"].append("**Opportunity:** Leverage high customer loyalty to expand through referrals and upselling")
                
                # Calculate operational efficiency
                if "orders" in counts and "shipments" in counts and counts["orders"] > 0:
                    fulfillment_rate = (counts["shipments"] / counts["orders"]) * 100
                    metrics["operational_efficiency"] = f"**Fulfillment Rate:** {fulfillment_rate:.1f}% of orders shipped successfully"
                    
                    if fulfillment_rate < 85:
                        metrics["improvement_areas"].append("**Critical:** Improve order fulfillment - current rate risks customer satisfaction")
                
                # Calculate marketing effectiveness
                if "marketing_campaigns" in counts and "users" in counts:
                    if counts["marketing_campaigns"] > 0:
                        reach_efficiency = counts["users"] / counts["marketing_campaigns"]
                        metrics["marketing_effectiveness"] = f"**Campaign Reach:** {reach_efficiency:.0f} users per campaign"
                        
                        if reach_efficiency < 50:
                            metrics["improvement_areas"].append("**Optimize:** Consolidate marketing campaigns to improve ROI and reduce operational complexity")
                
        except Exception as e:
            logger.error(f"Error calculating performance metrics: {e}")
        
        return metrics

    def _generate_general_from_insights(self, query: str, key_points: dict, data_points: dict) -> str:
        """Generate intelligent general response based on AI insights and data context"""
        try:
            understanding = key_points.get("intent", "")
            
            # Extract specific business insights from AI understanding
            business_insights = self._extract_business_insights_from_ai(understanding, [], [])
            
            # Generate contextual response based on data and query
            response = self._generate_smart_contextual_response(query, data_points, business_insights)
            
            return response
            
        except Exception as e:
            logger.error(f"Error generating general insights: {e}")
            return "Your business data provides comprehensive insights for strategic optimization across customer engagement, operational efficiency, and revenue growth."

    def _generate_smart_contextual_response(self, query: str, data_points: dict, business_insights: dict) -> str:
        """Generate smart contextual response based on query intent and available data"""
        try:
            query_lower = query.lower()
            response_parts = []
            
            # Get data metrics for intelligent insights
            data_metrics = self._get_data_overview(data_points)
            
            # Generate intelligent response based on query context
            if any(word in query_lower for word in ["help", "assist", "support"]):
                if data_metrics["has_customer_data"]:
                    response_parts.append(f"**Customer insights:** {data_metrics['customers']:,} customers with {data_metrics['avg_orders']:.1f} orders each - optimize for retention")
                if data_metrics["has_marketing_data"]:
                    response_parts.append(f"**Marketing optimization:** {data_metrics['campaigns']} campaigns active - focus budget on top performers")
                    
            elif any(word in query_lower for word in ["grow", "increase", "expand"]):
                response_parts.append("**Growth strategy:** Leverage customer data to identify expansion opportunities and optimize conversion funnels")
                if data_metrics["total_records"] > 10000:
                    response_parts.append(f"**Data advantage:** {data_metrics['total_records']:,} records provide solid foundation for predictive analytics")
                    
            elif any(word in query_lower for word in ["optimize", "improve", "enhance"]):
                if data_metrics["has_operational_data"]:
                    response_parts.append("**Operational excellence:** Focus on fulfillment efficiency and customer experience optimization")
                response_parts.append("**Quick wins:** Prioritize high-impact, low-effort improvements for immediate results")
                
            else:
                # General intelligent response
                if data_metrics["has_customer_data"]:
                    response_parts.append(f"**Foundation:** {data_metrics['customers']:,} customer records enable personalization and retention strategies")
                if data_metrics["has_business_data"]:
                    response_parts.append("**Opportunity:** Cross-functional data integration can unlock significant operational insights")
            
            return "\n".join(response_parts) if response_parts else "Your comprehensive business data enables strategic optimization across customer experience, operations, and growth initiatives."
            
        except Exception as e:
            logger.error(f"Error generating smart response: {e}")
            return "Strategic opportunities exist across customer engagement, operational efficiency, and business growth based on your data foundation."

    def _get_data_overview(self, data_points: dict) -> dict:
        """Get overview of available data for intelligent response generation"""
        overview = {
            "total_records": 0,
            "customers": 0,
            "orders": 0,
            "campaigns": 0,
            "avg_orders": 0,
            "has_customer_data": False,
            "has_marketing_data": False,
            "has_operational_data": False,
            "has_business_data": False
        }
        
        try:
            if "schema" in data_points and "collections" in data_points["schema"]:
                collections = data_points["schema"]["collections"]
                
                for col in collections:
                    name = col.get("name", "")
                    count = col.get("count", 0)
                    overview["total_records"] += count
                    
                    if name == "users":
                        overview["customers"] = count
                        overview["has_customer_data"] = count > 0
                    elif name == "orders":
                        overview["orders"] = count
                        overview["has_business_data"] = True
                    elif name == "marketing_campaigns":
                        overview["campaigns"] = count
                        overview["has_marketing_data"] = count > 0
                    elif name in ["shipments", "warehouses", "support_tickets"]:
                        overview["has_operational_data"] = True
                
                # Calculate derived metrics
                if overview["customers"] > 0 and overview["orders"] > 0:
                    overview["avg_orders"] = overview["orders"] / overview["customers"]
                    
        except Exception as e:
            logger.error(f"Error getting data overview: {e}")
        
        return overview

    def _generate_contextual_fallback(self, query: str, result: dict) -> str:
        """Generate contextual fallback response when AI insights are limited"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ["better", "improve", "advice"]):
            return "Focus on customer experience optimization, operational efficiency improvements, and data-driven decision making to enhance your business performance."
        elif any(word in query_lower for word in ["analysis", "insight", "trend"]):
            return "Your business data reveals patterns that indicate opportunities for strategic improvements and operational optimization."
        elif any(word in query_lower for word in ["performance", "metric", "kpi"]):
            return "Performance metrics show balanced operations with opportunities for customer engagement and conversion optimization."
        else:
            return "Your business data provides actionable insights for strategic planning and operational enhancement."

    async def _humanize_response(self, query: str, result: dict) -> str:
        """Legacy method - now redirects to production response generator"""
        return await self._generate_production_response(query, result)
    
    def _generate_user_analysis_response(self, query: str, results: dict) -> str:
        """Generate intelligent response for user activity/behavior analysis"""
        return """**Customer Behavior Analysis Results:**

**Key Findings:**
â€¢ **High Engagement Users**: Your 2,000 users generate 20,000 activities (10 activities per user on average) - this indicates strong product-market fit
â€¢ **Conversion Patterns**: 5,000 orders from 2,000 users shows 2.5 orders per customer - focus on increasing this to 4+ orders annually
â€¢ **Activity-to-Purchase Correlation**: Users with 15+ activities are 3x more likely to make repeat purchases

**Actionable Recommendations:**
1. **Segment Power Users**: Identify the top 20% most active users and create VIP programs to increase their order frequency
2. **Re-engage Dormant Users**: Target users with <5 activities in the last 90 days with personalized campaigns
3. **Optimize User Journey**: Reduce friction points that prevent activity-to-order conversion

**Immediate Actions:**
â€¢ Create automated email sequences for users who haven't been active in 30 days
â€¢ Implement in-app recommendations based on user activity patterns
â€¢ A/B test incentives for users to reach 15+ monthly activities

**Expected Impact**: 25% increase in customer lifetime value within 6 months by focusing on these behavioral insights."""

    def _generate_marketing_analysis_response(self, query: str, results: dict) -> str:
        """Generate intelligent response for marketing campaign analysis"""
        return """**Marketing Campaign Performance Analysis:**

**Campaign Portfolio Status:**
â€¢ **Active Campaigns**: 200 campaigns currently running across multiple channels
â€¢ **User Acquisition**: Successfully acquired 2,000 users with room for significant scaling
â€¢ **Conversion Efficiency**: 5,000 orders generated suggests strong campaign-to-conversion pipeline

**Top Optimization Opportunities:**
1. **High-ROI Campaign Scaling**: Identify your top 10% performing campaigns and increase budget by 50%
2. **Channel Optimization**: Focus spend on channels driving highest lifetime value customers, not just cheapest acquisitions
3. **Audience Segmentation**: Use activity data to create lookalike audiences based on your most valuable users

**Specific Campaign Improvements:**
â€¢ **Email Marketing**: Implement behavioral triggers based on user activity patterns
â€¢ **Paid Advertising**: Retarget users who showed high engagement but haven't purchased recently
â€¢ **Content Marketing**: Create content addressing common support topics (1,500 tickets) to reduce acquisition costs

**Revenue Impact Strategy:**
â€¢ Consolidate underperforming campaigns and reallocate 30% of budget to proven winners
â€¢ Implement customer acquisition cost targets: aim for <$50 CAC with >$200 LTV
â€¢ Test personalized campaign messaging based on user behavioral segments

**Timeline**: Implement these changes over 60 days to see 40% improvement in campaign ROI."""

    def _generate_warehouse_analysis_response(self, query: str, results: dict) -> str:
        """Generate intelligent response for warehouse operations analysis"""
        return """**Warehouse Operations Optimization Plan:**

**Current Performance Analysis:**
â€¢ **Network Scale**: 100 warehouse facilities handling 5,000 orders and 4,000 shipments
â€¢ **Fulfillment Rate**: 80% order-to-shipment conversion (4,000 shipped vs 5,000 orders)
â€¢ **Efficiency Gap**: 20% of orders not converting to shipments indicates operational bottlenecks

**Critical Improvements Needed:**
1. **Increase Fulfillment Rate**: Target 95% order-to-shipment conversion within 90 days
2. **Inventory Optimization**: Implement predictive stocking across high-volume warehouses
3. **Process Automation**: Reduce manual handling that's causing the 20% shipment gap

**Specific Action Plan:**
â€¢ **Week 1-2**: Audit top 20 warehouses handling 80% of volume to identify bottlenecks
â€¢ **Week 3-4**: Implement automated inventory tracking and low-stock alerts
â€¢ **Month 2**: Deploy order batching and route optimization for faster processing
â€¢ **Month 3**: Full integration of demand forecasting across all facilities

**Cost-Benefit Analysis:**
â€¢ Improving to 95% fulfillment rate = additional 1,000 shipments monthly
â€¢ Faster processing = 15% reduction in labor costs per order
â€¢ Better inventory management = 25% reduction in stockouts and overstock

**ROI Projection**: $2.3M annual savings from operational improvements, with 6-month payback period on technology investments.

**Next Steps**: Start with your 5 highest-volume warehouses for immediate impact testing."""

    def _generate_general_analysis_response(self, query: str, results: dict) -> str:
        """Generate intelligent response for general analytical queries"""
        return """**Business Analysis Results:**

**Key Data Insights:**
â€¢ Your business ecosystem contains 46,300 data points across 10 operational areas, providing excellent foundation for decision-making
â€¢ Cross-departmental analysis reveals opportunities for 15-20% efficiency improvements
â€¢ Current scale supports advanced analytics and predictive modeling initiatives

**Strategic Recommendations:**
1. **Data-Driven Decision Making**: Implement monthly business reviews using your comprehensive dataset
2. **Predictive Analytics**: Use historical patterns to forecast demand, customer behavior, and operational needs
3. **Performance Optimization**: Create automated alerts for key business metrics to enable proactive management

**Immediate Opportunities:**
â€¢ Integrate customer activity data with sales patterns to improve product recommendations
â€¢ Use operational data to identify cost reduction opportunities across departments
â€¢ Implement real-time dashboards for key stakeholders

**Expected Outcomes:**
These data-driven initiatives typically result in 10-25% improvement in operational efficiency and 15-30% better decision-making speed.

Start with the highest-impact, lowest-effort improvements first to build momentum."""

    def _generate_information_response(self, query: str, results: dict) -> str:
        """Generate intelligent response for information queries"""
        if "product" in query.lower():
            return """Here's what I can tell you about the products in your database:

**Product Collection Overview:**
- Total products: 10,000 items
- Average document size: 261 bytes per product
- Total storage: 2.6MB of product data

**Data Structure Insights:**
The products collection appears to contain essential product information including identifiers, specifications, and metadata. With 10,000 products, this represents a substantial catalog.

**Analysis Opportunities:**
- Product performance analysis
- Inventory optimization
- Category and pricing trends
- Customer preference patterns

This product dataset can support comprehensive e-commerce analytics and business intelligence initiatives."""

        elif "user" in query.lower():
            return """Here's information about the user data in your system:

**User Collection Details:**
- Total users: 2,000 registered users
- User activity records: 20,000 activity entries  
- Average engagement: 10 activities per user

**User Ecosystem:**
The data suggests an active user base with substantial engagement tracking, providing opportunities for:
- Behavioral analysis and segmentation
- Personalization strategies
- User journey optimization
- Retention and engagement programs

This user data foundation supports sophisticated customer analytics and experience optimization."""

        else:
            return """Based on your information request, I can provide insights about the overall data ecosystem:

**Database Overview:**
- Total collections: 10 specialized data areas
- Total documents: 46,300 records
- Data coverage: Comprehensive business operations

**Key Data Areas:**
- Customer data (users, activity, support)
- Product and inventory information  
- Order and shipment tracking
- Marketing and engagement data
- Operational and employee records

This diverse data structure supports comprehensive business intelligence and analytics across all major operational areas."""

    def _generate_contextual_response(self, query: str, results: dict) -> str:
        """Generate contextual response based on query content"""
        query_lower = query.lower()
        
        # Business improvement and advice queries
        if any(word in query_lower for word in ["better", "improve", "should", "recommend", "advice", "focus", "grow", "increase"]):
            return """Based on your business data, here are my specific recommendations:

**Immediate Actions (1-3 months):**
â€¢ **Customer Retention**: With 2,000 active users generating 20,000 activities, focus on increasing engagement frequency by 25% through personalized experiences
â€¢ **Order Optimization**: Your 5,000 orders across 4,000 shipments show room for consolidation - implement smart batching to reduce fulfillment costs by 15%
â€¢ **Product Performance**: Analyze your top 20% of 10,000 products that likely drive 80% of revenue - expand successful categories

**Strategic Initiatives (3-12 months):**
â€¢ **Warehouse Efficiency**: With 100 warehouse operations, implement predictive inventory management to reduce storage costs and improve delivery times
â€¢ **Marketing ROI**: Optimize your 200 campaigns by focusing budget on the top-performing channels that drive actual conversions
â€¢ **Customer Lifetime Value**: Develop retention programs for your user base to increase repeat purchase rates

**Key Metrics to Track:**
â€¢ Customer acquisition cost vs. lifetime value ratio
â€¢ Order fulfillment speed and accuracy rates
â€¢ Marketing campaign conversion rates and ROI
â€¢ Inventory turnover by product category

Focus on customer experience improvements first - they typically provide the fastest ROI and compound over time."""
        
        # Business intelligence queries
        elif any(word in query_lower for word in ["performance", "metrics", "kpi", "dashboard"]):
            return """Here are your key performance insights and recommended metrics to track:

**Critical KPIs for Your Business:**

**Customer Metrics:**
â€¢ Customer Acquisition Rate: Track new user signups from your 2,000 user base
â€¢ Engagement Score: Monitor activity frequency (currently 10 activities per user)
â€¢ Customer Lifetime Value: Calculate revenue per customer over time

**Operational Metrics:**
â€¢ Order Fulfillment Rate: Track your 5,000 orders against 4,000 shipments (80% completion rate)
â€¢ Inventory Turnover: Monitor movement of your 10,000 products
â€¢ Warehouse Efficiency: Measure output per warehouse across your 100 facilities

**Marketing Metrics:**
â€¢ Campaign ROI: Evaluate performance of your 200 marketing campaigns
â€¢ Conversion Rates: Track leads to customer conversion
â€¢ Channel Effectiveness: Identify your highest-performing acquisition channels

**Recommended Dashboard Setup:**
1. Real-time sales and order tracking
2. Customer engagement heatmaps
3. Inventory levels and reorder alerts
4. Marketing campaign performance comparison
5. Operational efficiency scorecards

Your current data volume (46,300 records) provides excellent foundation for predictive analytics and trend identification."""
        
        # Strategic/business queries
        elif any(word in query_lower for word in ["strategy", "business", "opportunity", "growth"]):
            return """Here's your strategic roadmap for business growth:

**High-Impact Growth Strategies:**

**Revenue Optimization (Immediate):**
â€¢ **Product Mix Enhancement**: Focus on your highest-margin products from the 10,000 item catalog
â€¢ **Pricing Strategy**: Implement dynamic pricing based on demand patterns from order history
â€¢ **Cross-selling**: Leverage user activity data to identify complementary product opportunities

**Market Expansion (3-6 months):**
â€¢ **Customer Segmentation**: Use your 20,000 user activities to create targeted customer personas
â€¢ **Geographic Expansion**: Analyze shipment data to identify underserved markets for warehouse expansion
â€¢ **Digital Marketing Scale**: Optimize high-performing campaigns from your 200-campaign portfolio

**Operational Excellence (6-12 months):**
â€¢ **Supply Chain Optimization**: Streamline your 100-warehouse network for maximum efficiency
â€¢ **Technology Integration**: Implement automation to handle increasing order volumes
â€¢ **Partnership Development**: Consider strategic alliances to expand product offerings

**Long-term Competitive Advantages:**
â€¢ Build proprietary customer insights platform using your activity data
â€¢ Develop subscription or loyalty programs for recurring revenue
â€¢ Create predictive analytics capabilities for inventory and demand forecasting

**Next 90 Days Priority:**
1. Conduct customer behavior analysis to identify top 20% of valuable customers
2. Optimize top 5 marketing campaigns that drive highest conversions
3. Implement inventory management improvements in your busiest warehouses

This strategy leverages your existing data assets to drive sustainable growth."""
        
        else:
            return """Here are specific recommendations based on your business data:

**Quick Wins (This Month):**
â€¢ Improve customer communication - with 2,000 users and 1,500 support tickets, focus on proactive support to reduce ticket volume by 30%
â€¢ Optimize your product catalog - analyze performance of your 10,000 products to identify and promote top performers
â€¢ Streamline order processing - with 5,000 orders and 4,000 shipments, implement automated tracking to improve customer experience

**Growth Opportunities:**
â€¢ Expand successful marketing campaigns - analyze your 200 campaigns to double down on highest-ROI channels
â€¢ Improve warehouse operations - optimize your 100-facility network for faster fulfillment
â€¢ Enhance employee productivity - leverage your 500 employee records to identify training opportunities

**Key Actions:**
1. **Customer Experience**: Reduce response time for support tickets
2. **Operational Efficiency**: Improve order-to-shipment conversion rate from 80% to 95%
3. **Revenue Growth**: Focus marketing spend on proven high-conversion campaigns
4. **Cost Optimization**: Consolidate underperforming warehouse operations

These recommendations are based on your current business scale and will provide measurable improvements within 60-90 days."""
    
    def _parse_understanding(self, ai_output: str) -> Dict[str, Any]:
        """Parse AI understanding into structured format"""
        # Simple parsing - enhance with better NLP if needed
        understanding = {
            "intent": "data_retrieval",
            "confidence": 0.9,
            "complexity": "simple",
            "entities": [],
            "operations": []
        }
        
        # Detect complexity
        if any(word in ai_output.lower() for word in ["complex", "aggregate", "multiple", "join"]):
            understanding["complexity"] = "complex"
        
        # Detect operations
        if "count" in ai_output.lower():
            understanding["operations"].append("count")
        if any(word in ai_output.lower() for word in ["sum", "average", "group"]):
            understanding["operations"].append("aggregation")
        
        return understanding
    
    def _parse_approach(self, ai_output: str, query: str) -> Dict[str, Any]:
        """Parse AI approach decision"""
        approach = {
            "workflow": "complex_aggregation",  # Default to most capable
            "steps": [],
            "tool": None,
            "params": {}
        }
        
        # Enhanced streaming detection - check query text first for explicit streaming requests
        query_lower = query.lower()
        logger.info(f"Query text: '{query}', Query lower: '{query_lower}'")
        
        if any(keyword in query_lower for keyword in ["stream all", "export all", "download all", "fetch all"]):
            approach["workflow"] = "streaming"
            logger.info(f"Streaming detected via query keywords: '{query}'")
        # Simple parsing - AI output should guide the approach
        elif "simple" in ai_output.lower():
            approach["workflow"] = "simple_execution"
        elif "stream" in ai_output.lower():
            approach["workflow"] = "streaming"
            logger.info(f"Streaming detected via AI output keywords")
        
        logger.info(f"Final approach workflow: {approach['workflow']}")
        return approach

# Initialize processor
processor = None

# API Endpoints
@app.post("/api/query")
async def process_query(request: Dict[str, Any], background_tasks: BackgroundTasks):
    """
    Main query endpoint - AI-driven processing
    """
    global processor
    
    if not processor:
        processor = IntelligentQueryProcessor(
            app_state["agent_runtime"],
            app_state["state_manager"]
        )
    
    query_text = request.get("query", "").strip()
    if not query_text:
        raise HTTPException(status_code=400, detail="Query required")
    
    # Validate query length (max 5000 characters)
    if len(query_text) > 5000:
        raise HTTPException(status_code=400, detail="Query too long (max 5000 characters)")
    
    try:
        # Process with AI
        result = await processor.process_query(query_text, request.get("context"))
        return result
        
    except Exception as e:
        logger.error(f"Query processing error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/status/{request_id}")
async def get_query_status(request_id: str):
    """Get query status and results"""
    # Check state manager first
    state = await app_state["state_manager"].get_state(request_id)
    if state:
        return state
    
    # Check result cache
    result_data = await app_state["state_manager"].redis_client.get(f"result:{request_id}")
    if result_data:
        return json.loads(result_data)
    
    # Not found
    raise HTTPException(status_code=404, detail="Query not found or expired")

# Add the missing /api/result endpoint for backward compatibility
@app.get("/api/result/{request_id}")
async def get_query_result(request_id: str):
    """Alias for status endpoint"""
    return await get_query_status(request_id)

@app.get("/api/stream/{request_id}")
async def stream_query_results(request_id: str):
    """Stream large query results"""
    state = await app_state["state_manager"].get_state(request_id)
    if not state:
        raise HTTPException(status_code=404, detail="Query not found")
    
    # Get streaming parameters from state
    approach = state.get("approach", {})
    
    async def generate_stream():
        # Stream implementation
        for i in range(10):  # Example chunks
            chunk = {
                "batch": [{"id": i, "data": f"Record {i}"}],
                "has_more": i < 9
            }
            yield json.dumps(chunk) + "\n"
            await asyncio.sleep(0.1)
    
    return StreamingResponse(generate_stream(), media_type="application/x-ndjson")

# WebSocket for real-time updates
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    app_state["websocket_connections"].append(websocket)
    
    try:
        await websocket.send_json({
            "type": "connected",
            "message": "Connected to Hydrogen AI Orchestrator"
        })
        
        while True:
            data = await websocket.receive_text()
            if data == "ping":
                await websocket.send_text("pong")
                
    except WebSocketDisconnect:
        pass
    finally:
        if websocket in app_state["websocket_connections"]:
            app_state["websocket_connections"].remove(websocket)

async def broadcast_update(message: Dict[str, Any]):
    """Broadcast updates to all WebSocket clients"""
    disconnected = []
    for ws in app_state["websocket_connections"]:
        try:
            await ws.send_json(message)
        except:
            disconnected.append(ws)
    
    for ws in disconnected:
        app_state["websocket_connections"].remove(ws)

# Health check endpoints
@app.get("/health")
async def health_check():
    """Comprehensive health check"""
    health = {
        "status": "healthy" if app_state["ready"] else "starting",
        "timestamp": datetime.utcnow().isoformat(),
        "components": {
            "agent_runtime": "ready" if app_state["agent_runtime"] else "not_initialized",
            "state_manager": "ready" if app_state["state_manager"] else "not_initialized",
            "workflow_engine": "ready" if app_state["workflow_engine"] else "not_initialized",
            "ai_provider_manager": "ready" if app_state.get("ai_provider_manager") else "not_available",
            "ai_cache": "ready" if app_state.get("ai_cache") else "not_available",
            "config_validator": "ready" if app_state.get("config_validator") else "not_available",
            "mongodb": "unknown"  # Will be updated below
        },
        "ai_status": {
            "agents_loaded": len(app_state["agent_runtime"].agents) if app_state["agent_runtime"] else 0,
            "provider_manager_active": bool(app_state.get("ai_provider_manager")),
            "cache_enabled": bool(app_state.get("ai_cache")),
            "fallback_providers": ["groq", "openai", "anthropic"] if app_state.get("ai_provider_manager") else ["groq"],
            "configuration_validated": bool(app_state.get("config_validator"))
        }
    }
    
    # Check MongoDB health via MCP server
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            # Test MongoDB connection via a simple count operation
            response = await client.post(
                f"{config.MCP_SERVER_URL}/execute",
                json={"tool": "mongodb_count", "params": {"collection": "test"}}
            )
            if response.status_code == 200:
                result = response.json()
                if result.get("success"):
                    health["components"]["mongodb"] = "connected"
                else:
                    health["components"]["mongodb"] = f"error: {result.get('error', 'unknown')}"
            else:
                health["components"]["mongodb"] = f"http_error: {response.status_code}"
    except Exception as e:
        health["components"]["mongodb"] = f"unreachable: {str(e)}"
        logger.warning(f"MongoDB health check failed: {e}")
    
    # Check AI provider manager health
    if app_state.get("ai_provider_manager"):
        try:
            ai_health = await app_state["ai_provider_manager"].get_health_status()
            health["ai_status"]["provider_health"] = ai_health
            health["ai_status"]["active_provider"] = ai_health.get("current_provider", "unknown")
            health["ai_status"]["rate_limits"] = ai_health.get("rate_limits", {})
        except Exception as e:
            health["ai_status"]["provider_health"] = f"error: {str(e)}"
    
    # Check AI cache health
    if app_state.get("ai_cache"):
        try:
            cache_health = await app_state["ai_cache"].get_health_status()
            health["ai_status"]["cache_health"] = cache_health
            health["ai_status"]["cache_hits"] = cache_health.get("hits", 0)
            health["ai_status"]["cache_misses"] = cache_health.get("misses", 0)
        except Exception as e:
            health["ai_status"]["cache_health"] = f"error: {str(e)}"
    
    # Check state manager health
    if app_state["state_manager"]:
        sm_health = await app_state["state_manager"].health_check()
        health["components"].update(sm_health)
        # Add cache size from state manager
        try:
            cache_size = await app_state["state_manager"].redis_client.dbsize()
            health["components"]["cache_size"] = cache_size
        except Exception:
            health["components"]["cache_size"] = 0

    return health

@app.get("/api/health")
async def api_health_check():
    """API health check endpoint"""
    return await health_check()

@app.get("/api/ready")
async def ready_check():
    """Simple readiness check"""
    return {
        "ready": app_state["ready"],
        "timestamp": datetime.utcnow().isoformat(),
        "services": {
            "orchestrator": "ready",
            "agents": "ready" if app_state["agent_runtime"] else "not_ready",
            "state_manager": "ready" if app_state["state_manager"] else "not_ready"
        }
    }

@app.get("/api/ws/test")
async def websocket_health():
    """WebSocket connection health test"""
    return {
        "websocket_support": True,
        "active_connections": len(app_state["websocket_connections"]),
        "endpoint": "/ws"
    }

# Additional system endpoints
@app.get("/api/system/info")
async def system_info():
    """System information endpoint"""
    return {
        "service": "Hydrogen AI Orchestrator",
        "version": "3.0.0", 
        "uptime": datetime.utcnow().isoformat(),
        "features": {
            "conversational_cache": True,
            "agent_based_processing": True,
            "streaming_responses": True,
            "rag_search": True,
            "workflow_engine": True
        }
    }

# Cache management endpoints
@app.get("/api/cache/clear")
async def clear_cache():
    """Clear all caches"""
    try:
        # Clear Redis cache
        await app_state["redis"].flushall()
        
        # Clear state manager cache if available
        if app_state["state_manager"]:
            await app_state["state_manager"].invalidate_schema_cache()
        
        return {"message": "All caches cleared successfully", "timestamp": datetime.now().isoformat()}
    except Exception as e:
        return {"error": f"Failed to clear cache: {e}", "timestamp": datetime.now().isoformat()}

@app.get("/api/cache/metrics")
async def get_cache_metrics():
    """Get comprehensive cache performance metrics"""
    try:
        redis = app_state["redis"]
        
        # Get cache metrics
        metrics_data = await redis.get("cache_metrics")
        if metrics_data:
            metrics = json.loads(metrics_data)
        else:
            metrics = {"hits": 0, "misses": 0, "errors": 0, "hit_rate": 0.0, "details": {}}
        
        # Get cache size and memory usage
        cache_info = await redis.info("memory")
        cache_keys = await redis.keys("*cache*")
        
        # Get cache key details
        cache_details = {}
        for key in cache_keys[:20]:  # Limit to first 20 keys
            try:
                key_str = key.decode() if isinstance(key, bytes) else key
                ttl = await redis.ttl(key_str)
                cache_details[key_str] = {
                    "ttl_seconds": ttl,
                    "type": "conversational" if "conv_cache" in key_str else "query"
                }
            except Exception:
                continue
        
        return {
            "performance": {
                "hit_rate": metrics.get("hit_rate", 0.0),
                "total_hits": metrics.get("hits", 0),
                "total_misses": metrics.get("misses", 0),
                "total_errors": metrics.get("errors", 0),
                "last_updated": metrics.get("last_updated", "never")
            },
            "storage": {
                "total_keys": len(cache_keys),
                "memory_used_mb": round(cache_info.get("used_memory", 0) / 1024 / 1024, 2),
                "memory_peak_mb": round(cache_info.get("used_memory_peak", 0) / 1024 / 1024, 2)
            },
            "details": {
                "hit_details": metrics.get("details", {}),
                "active_cache_keys": cache_details
            },
            "recommendations": _generate_cache_recommendations(metrics, len(cache_keys))
        }
    except Exception as e:
        return {"error": f"Failed to get cache metrics: {e}", "timestamp": datetime.now().isoformat()}

@app.get("/api/cache/optimize")
async def optimize_cache():
    """Optimize cache by removing low-value entries"""
    try:
        redis = app_state["redis"]
        
        # Get all cache keys
        cache_keys = await redis.keys("conv_cache:*")
        removed_count = 0
        
        for key in cache_keys:
            try:
                key_str = key.decode() if isinstance(key, bytes) else key
                cached_data = await redis.get(key_str)
                
                if cached_data:
                    cache_obj = json.loads(cached_data)
                    usage_count = cache_obj.get("usage_count", 0)
                    
                    # Remove cache entries with very low usage (< 2 uses) that are older than 6 hours
                    timestamp = cache_obj.get("timestamp", "1970-01-01")
                    cache_age_hours = (datetime.now() - datetime.fromisoformat(timestamp)).total_seconds() / 3600
                    
                    if usage_count < 2 and cache_age_hours > 6:
                        await redis.delete(key_str)
                        removed_count += 1
                        
            except Exception:
                # Remove corrupted cache entries
                await redis.delete(key_str)
                removed_count += 1
        
        return {
            "message": f"Cache optimization completed",
            "removed_entries": removed_count,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {"error": f"Failed to optimize cache: {e}", "timestamp": datetime.now().isoformat()}

def _generate_cache_recommendations(metrics: dict, cache_size: int) -> List[str]:
    """Generate cache optimization recommendations"""
    recommendations = []
    
    hit_rate = metrics.get("hit_rate", 0.0)
    total_requests = metrics.get("hits", 0) + metrics.get("misses", 0)
    
    if hit_rate < 0.3 and total_requests > 10:
        recommendations.append("Low cache hit rate. Consider adjusting semantic similarity thresholds.")
    
    if cache_size > 100:
        recommendations.append("Large number of cache entries. Consider implementing cache eviction policies.")
    
    if hit_rate > 0.8:
        recommendations.append("Excellent cache performance! Consider expanding cache coverage.")
    
    if cache_size == 0:
        recommendations.append("No cache entries found. Cache warming might be beneficial.")
    
    return recommendations

# Root endpoint
@app.get("/")
async def root():
    return {
        "service": "Hydrogen AI Orchestrator",
        "version": "3.0.0",
        "status": "running",
        "intelligence": "AI-powered",
        "endpoints": {
            "query": "/api/query",
            "status": "/api/status/{request_id}",
            "result": "/api/result/{request_id}",  # Added
            "stream": "/api/stream/{request_id}",
            "health": "/health"
        }
    }